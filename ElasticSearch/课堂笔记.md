# 上课笔记

## 1. 全文检索概念与特点

**结构化数据**: 主要以`关系型数据库表`形式管理的数据,比如MySQL中的行和列

**半结构化数据**: 有基本固定结构模式的数据，例如日志文件、XML文档、JSON文档、Email等。

**非结构化数据**: 没有固定模式的数据，如WORD、PDF、PPT、EXL，各种格式的图片、视频等。

> 理解:可以理解为全文检索就是把没有结构化的数据变成有结构的数据,然后进行搜索,因为有结构化的数据通常情况下可以按照某种算法进行搜索!

特点:

1. 匹配度最高的排在最前面
2. 搜索的关键词高亮
3. 只处理文本不处理语义。也就是仅仅列出包含了搜索关键字的网页！

## 2. Lucene底层核心实现原理

**Lucene**:是一个开源的`全文检索引擎工具包`---->说白了就是一堆jar包

**核心**:索引创建、索引搜索

理解:索引创建就是指将无结构的数据转换为Lucene的某种有结构的数据,而索引搜索就是指从有结构数据中去搜索!

整体架构:

![img](https://gitee.com/miawei/pic-go-img/raw/master/imgs/wps7AF4.tmp.jpg)

**理解**:

​	Index索引库里存放着有结构的数据也就是索引。从中间一分为二左边就是创建索引而右边就是搜索索引的过程；首先从左边开始，首先就要采集数据那么就有可能从原数据中获取也有可能从数据库中或者从WEB层也有可能手动输入，然后最终将采集的数据创建为Lucene的一个个文档，然后就把这个文档扔个Lucene它就会自己创建索引扔进这个索引库！而右边就开始搜索：比如搜索Java那么就会去索引的搜索，拿到对应的关键字就去索引库中搜，搜到了就会返回给页面展示！

> 创建索引的原理

接下来就来研究从采集数据到如何去创建索引并丢进索引库的过程：

![image-20211113195133463](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211113195133463.png)

![image-20211113200400756](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211113200400756.png)

​	首先1号是从数据库获取的数据文本右边有个编号可以认为那是id；按照以前的方式从数据库中模糊搜索那么就要like搜索，那么就要每个文本都要去匹配一下这样就会导致效率特别慢，而现在开始建立索引那么发生了什么变化呢？来看第二步：他就是将1号按照空格拆分然后竖着排列--》我们把这个过程称之为`分词`;然后看第3号第三步:将文本单词进行大小写转换(词态转换);第四步:进行单词排序(自然排序->ASC码表);第五步:将相同的单词进行合并,但是可能存在两个单词的id不一样那么就会将两个id挂载在相同的单词后面;

第五步也被我们称之为"倒排索引",而倒排索引是一种数据结构。

而为什么叫倒排索引呢？通常的正常索引一般是id对应一段文本数据，以id为准，而现在这个倒排索引则是将文本对应id，以文本为准，似乎感觉这就是倒着来的；倒排索引存在Lucene的索引库中，而Lucene索引库有两部分组成：**倒排索引**（索引区）、**原始数据**（数据区）

```bash
理解:所谓的倒排索引就是指存在于Lucene的索引库中主要用于创建索引;首先会将文本数据进行标记一个id,然后进行分词拆分并进行存放在列表中,然后将列表中的单词进行大小写转换,目的就是为了搜索时更加的方便;然后将整个列表进行自然排序也就是按照ASCll码表进行排序,排好顺序之后就会对重复的单词进行一个合并,那么这个时候就会有两个不同的id,那么这个时候就会将不同的id一起挂载到合并后的单词后面-->最终就形成了倒排索引!
	而在刚刚提到在将文本进行拆分的时候就有一个标记id,而这个id就是指在Lucene索引库中的数据区中的原始数据的id!
-----------------------------------------------------------------------------------------------------------------------------------------------------
总的来说文本数据会存有两份,一份就是转换后的倒排索引,另一种就是存在在数据区的原始数据!
```

注:为什么要进行大小写转换?目的就是在搜索的时候忽略大小写,全都转换成小写然后全都找出来!

> 索引搜索

来看看文本搜索的整个过程:

![image-20211113202905156](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211113202905156.png)

​	关键字搜索比如Java,那么就会到Lucene的索引区中的倒排索引搜索,而在数据列表中左边的文本称之为词典,而左边词典在创建索引的时候就已经按照自然排序进行排序过的,那么也就意味着在搜索的过程中可以按照某种二分查找算法快速的找到Java关键字,也就可以说不用去一个个去对比,找到Java关键字之后就会根据他挂载的id,根据id直接去数据区中找到对应的id的数据,将其返回!

```bash
理解:比如搜索Java首先就会去Lucene索引区中使用倒排索引查找,而在索引区中数据都是已经被自然排序过了,而基于Lucene的某种二分查找法快速找到Java关键字,找到之后根据对应挂载的id去数据区中找到原始数据,然后将原始数据返回给用户展示界面!
--------------------------------------------------------------------------------------------------------------------------------------------
注意:找到关键字Java之后就不会往下去找了,因为文本单词在创建索引的时候就已经被合并了!
```

在上面可以看到搜索的关键字有java和world,那么在搜索到java对应的id是1,2然后将其返回,然后又会带着world去搜找到1,3然后又返回,有种感觉就是OR的意思!那么这里数据区3个文档都找出来了,那么再返回给用户展示的时候就会对查找的的关键字匹配度较高进行排列在前面,比如这里id为1的文档匹配找到了2次,所以说文档1就会排列在最前面!从这里看出匹配权重排序也是有一定的规则的!



**面试题:问:ES、Lucene为什么那么快?**

自我理解回答:

​	首先ES就是基于Lucene的基础上封装的,核心就是Lucene,而Lucene为什么查找那么快,是因为在Lucene的索引库中存放这索引区和数据区,在索引区中已经按照倒排索引对数据进行拆分、大小写转换、排序、合并,那么在搜索关键字的时候就会因为本身就是排序过的列表,然后根据某种二分查找法快速定位到匹配的关键字,而关键字挂载的有对应的id,而这id就是数据区中的id的原始数据,所以快速找到了之后就会将原始数据返回给用户进行展示!所以说为什么Lucene查找速度那么快的原因,而基于这种查找所以说ES也这么快!



**面试题:问:分词会在几个地方进行分词?**

自我理解回答:

​	两处:首先会在搜索的文本中会对其进行分词,比如"中国",那么就会拆分成"中","国",在分词之后才会去搜索,这是第一处,然后就是在Lucene的索引库中的索引区创建索引变成倒排索引的过程中对其分词拆分!这样也就是说用户在搜索的关键字就会对其拆分,"中国"拆分成"中","国",然后在索引区创建倒排索引过程比如文本"中国",拆分成"中","国",然后在搜索匹配的时候只要一匹配就会返回结果!



**面试题:问:所有字段都要建立倒排索引吗?**

自我理解回答:

​	不是,如果某个列要做关键字搜索,那么这个字段就要建立倒排索引!来提高查询速度!不可能对图片地址进行倒排索引查询撒!所以说我们要选择参与搜索的字段进行倒排索引



**面试题:问:表中的所有列都要放到数据区吗?**

自我理解回答:

​	不是,比如说数据库存放30个列字段,而页面展示只能用到10个列,那么这个时候就不需要用到那么多的列,只需要放用得到的就行!还有就是查询条件需要用到的数据

​	

**面试头:问:哪些字段需要倒排索引但是不需要分词？**

自我理解回答:

​	分词之后查询改变了单词本身含义，而通常情况下这种字段要进行整体搜索，比如“username”

### 2.1 入门代码

这个没那么重要,简单的写一下记录一下,万一以后用得到可以来看看:

```xml
<dependency>
    <groupId>org.apache.lucene</groupId>
    <artifactId>lucene-core</artifactId>
    <version>5.5.0</version>
</dependency>
<dependency>
    <groupId>org.apache.lucene</groupId>
    <artifactId>lucene-analyzers-common</artifactId>
    <version>5.5.0</version>
</dependency>
<dependency>
    <groupId>org.apache.lucene</groupId>
    <artifactId>lucene-queryparser</artifactId>
    <version>5.5.0</version>
</dependency>
```

1. 创建索引

   ![img](https://gitee.com/miawei/pic-go-img/raw/master/imgs/wps104.tmp.jpg)

   ```java
   /**
   	步骤:
   	1、把文本内容转换为Document对象
   	2、准备IndexWriter（索引写入器）
   	3、通过IndexWriter，把Document添加到缓
   */
   //创建索引的数据 现在写死，以后根据实际应用场景
   String doc1 = "hello world";
   String doc2 = "hello java world";
   String doc3 = "hello lucene world";
   private String path ="F:/eclipse/workspace/lucene/index/hello";
   @Test
   public void testCreate() {
   	try {
   		//2、准备IndexWriter（索引写入器）
   		//索引库的位置 FS fileSystem
   		Directory d = FSDirectory.open(Paths.get(path ));
   		//分词器 -- 把文档进行分词
   		Analyzer analyzer = new SimpleAnalyzer();
   		//索引写入器的配置对象
   		IndexWriterConfig conf = new IndexWriterConfig(analyzer);
   
   		IndexWriter indexWriter = new IndexWriter(d, conf);
   		System.out.println(indexWriter);
   		
   		//1、 把文本内容转换为Document对象
   		//把文本转换为document对象
   		Document document1 = new Document();
   		//标题字段
   		document1.add(new TextField("title", "doc1", Store.YES));
   		document1.add(new TextField("content", doc1, Store.YES));
   		//添加document到缓冲区
   		indexWriter.addDocument(document1);
   
   		Document document2 = new Document();
   		//标题字段
   		document2.add(new TextField("title", "doc2", Store.YES));
   		document2.add(new TextField("content", doc2, Store.YES));
   		//添加document到缓冲区
   		indexWriter.addDocument(document2);
   
   		Document document3 = new Document();
   		//标题字段
   		document3.add(new TextField("title", "doc3", Store.YES));
   		document3.add(new TextField("content", doc3, Store.YES));
   			
   		//3 、通过IndexWriter，把Document添加到缓冲区并提交
   		//添加document到缓冲区
   		indexWriter.addDocument(document3);
   		indexWriter.commit();
   	    indexWriter.close();
   			
   	} catch (Exception e) {
   		e.printStackTrace();
   	}
   }
   ```

2. 搜索索引:

   ![image-20211113211524344](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211113211524344.png)

   ```java
   /**
   	1 封装查询提交为查询对象
   	2 准备IndexSearcher 
   	3 使用IndexSearcher传入查询对象做查询-----查询出来只是文档编号DocID
   	4 通过IndexSearcher传入DocID获取文档
   	5 把文档转换为前台需要的对
   */
   @Test
   public void testSearch() {
   	String keyWord = "lucene";
   	try {
   		// * 1 封装查询提交为查询对象
   	    //通过查询解析器解析一个字符串为查询对象
   		String f = "content"; //查询的默认字段名,
   		Analyzer a = new SimpleAnalyzer();//查询关键字要分词，所有需要分词器
   		QueryParser parser = new QueryParser(f, a);
   		Query query = parser.parse("content:"+keyWord); 
   
   		// * 2 准备IndexSearcher
           String path ="F:/eclipse/workspace/lucene/index/hello";
   		Directory d = FSDirectory.open(Paths.get(path ));
   		IndexReader r = DirectoryReader.open(d);
   		IndexSearcher searcher = new IndexSearcher(r);
   
   		// * 3 使用IndexSearcher传入查询对象做查询-----查询出来只是文档编号DocID
   		TopDocs topDocs = searcher.search(query, 1000);//查询ton条记录 前多少条记录
   		System.out.println("总命中数："+topDocs.totalHits);
   		ScoreDoc[] scoreDocs = topDocs.scoreDocs;//命中的所有的文档的封装（docId）
   
   		// * 4 通过IndexSearcher传入DocID获取文档
   		for (ScoreDoc scoreDoc : scoreDocs) {
   			int docId = scoreDoc.doc;
   			Document document = searcher.doc(docId);
   			// * 5 把文档转换为前台需要的对象 Docment----> Article
   			System.out.println("=======================================");
   			System.out.println("title:"+document.get("title")
   							+",content:"+document.get("content"));
   		}
   	} catch (Exception e) {
   		e.printStackTrace();
   	}
   }
   ```

   

## 3. ES

### 3.1.概念

​	Lucene可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。但是，Lucene`只是一个库`。想要使用它你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene的配置及使用非常`复杂`，你需要深入了解检索的相关知识来理解它是如何工作的。

ES的出现就是为了解决原生Lucene使用的不足,优化了Lucene的调用方式,并实现`高可用`的分布式集群的搜索方案!

> ES的是基于Lucene作为核心来实现索引和搜索的一个封装,它的目的就是通过简单的RESTFul风格来隐藏Lucene的复杂性,从而让全文搜索变得简单!

好处:

1. 分布式的**实时文件存储**,每个字段都被索引并可被搜索
2. 分布式的**实时分析搜索引擎**
3. 可以扩展上百台服务器,**处理PB级别的数据**(KB-MB-GB-TB-PB),
4. 高度集成化的服务,你的应用可以**通过简单的RESTFul API进行各种语言的客户端甚至命令行进行交互**!
5. 上手容易,学习成本低,安装即可用

### 3.2 节点信息

介绍三个节点信息:

![image-20211113213034160](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211113213034160.png)

- _index：索引库，类似于关系型数据库里的“数据库”—它是我们存储和索引关联数据的地方。
- _type：在应用中，我们使用对象表示一些“事物”，例如一个用户、一篇博客、一个评论，或者一封邮件。可以是大写或小写，不能包含下划线或逗号。我们将使用 employee 做为类型名
- _id:与 _index  和 _type  组合时，就可以在ELasticsearch中唯一标识一个文档。当创建一个文档，你可以自定义 _id  ，也可以让Elasticsearch帮你自动生成。
- _source：文档原始数据
- _all：所有字段的连接字符串

### 3.3 相关操作

> 批量操作-bulk

```bash
# 使用单一请求来实现多个文档的create、index、update 或 delete。
# Bulk请求体格式：
# { action: { metadata }}\n
# { request body }\n
# { action: { metadata }}\n
# { request body }\n
#每行必须以 "\n"  符号结尾，包括最后一行。这些都是作为每行有效的分离而做的标记。
#	create当文档不存在时创建之。
#	index创建新文档或替换已有文档。
#	update局部更新文档。
#	delete删除一个文档。
#例如：
POST _bulk
{ "delete": { "_index": "itsource", "_type": "employee", "_id": "123" }}
{ "create": { "_index": "itsource", "_type": "blog", "_id": "123" }}
{ "title": "我发布的博客" }
{ "index": { "_index": "itsource", "_type": "blog" , "_id": "456"}}
{ "title": "我的第二博客" }

#注意：delete后不需要请求体，最后一行要有回车
```

> 批量获取

```bash
方式1:
GET _mget
{
	"docs" : [
				{
                    "_index" : "itsource",
                    "_type" : "employee",
                    "_id" : 2
				},
				{
                    "_index" : "itsource",
                    "_type" : "dept",
                    "_id" : 1,
                    "_source": "email,age"  //只展示指定列
				}
              ]
}
方式2:
GET itsource/blog/_mget
{
	"ids" : [ "2", "1" ]
}
```

> 空搜索

```java
GET _search  //没有指定任何的查询条件，只返回集群索引中的所有文档
```

> 搜索时的参数列表

碧如:GET itsource/employee/_search?**q**=age:25&size=5&from=10

![image-20211113214347781](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211113214347781.png)

![image-20211113214355744](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211113214355744.png)

### 3.4 ES查询

DSL:由ES提供丰富且灵活的查询语言叫做DSL查询(Query DSL),它允许你构建更加复杂、强大的查询。DSL以**JSON请求体**的形式出现

> DSL模式

```java
GET itsource/employee/_search
{
        "query" : {
           "match" : { //match 标准查询(分词匹配),match会对关键字分词
                "username" : "公仔娃娃"
			}
		}
}
```

- **match**:标准查询(分词匹配)
- **Term**:词元匹配(把关键字看成整体去匹配)

理解:match在查询之前将搜索词进行分词拆分,Term不会对搜索词进行拆分,直接去搜索匹配

举例:

Match -> Where username = “公仔” or username = 娃娃

Term  -> Where username = “公仔娃娃”

> 通配符匹配

```java
GET itsource/employee/_search
{
  "query": {
        "wildcard" : { //通配符查询
 			"name" : "*zs*"  //只要name中出现了   zs的
        }
    }
}
```

举例:**Wildcard** -> Where name like “%zs%”

注意:使用*代表0~N个，使用?代表1个

> DSL有两部分组成：DSL查询和DSL过滤

DSL查询:普通的查询

DSL过滤:过滤一些比较精确的条件

```java
DSL过滤:  查询文档的方式更像是对于我的条件“有”或者“没有”，--精确查询
DSL查询:  则更像于"有多像"--类似模糊查询
---------------------------------------------------------------------------
两者区别:
1. 过滤结果可以缓存并应用到后续请求;
2. 查询语句同时 匹配文档，计算相关性，所以更耗时，且不缓存。
3. 过滤语句 可有效地配合查询语句完成文档过滤
```

举个例子:

```java
Get index/type/_search
{
    "query": {
       "bool": {  //BooleanQuery 组合查询
              "must": //查询 与(must) 或(should) 非(must not)
           		[   
                    { 
                        "match": {//标准查询(分词匹配)  term：单词查询(部分词)
                        "description": "hello java" 
                    	}
					}
				],
           	"filter": {  //过滤
               "term": {"description": "hello world"}
           	}
   	 	}
	},
    "from": 20, 
    "size": 10,
    "_source": ["fullName", "age", "email"],
    "sort": [{"join_date": "desc"},{"age": "asc"}]
}
```

> multi_match-多字段搜索

```java
//multi_match  查询允许你做 match查询的基础上同时搜索多个字段：
{
    "query":{
    	"multi_match": {
    		"query": "Steven King",
    		"fields": [ "fullName", "title" ]
    	}
    }
}
//上面的搜索同时在fullName和title字段中匹配
```

相当于:where fullName = “Steven ”or fullName = “King”or title= “Steven ”or title= “King”

> Terms-minimum_match

```java
{
	"query": {
		"terms": {
			"title": ["jvm", "hadoop", "lucene"], //同时搜索多个词
			"minimum_match": 2  //至少匹配个数,默认为1
		}
	}
}
```

terms:精准搜索多个词

> 组合查询-bool

注意:bool不要认为这是布尔值什么的,这是一个组合查询,也就是说可以同时使用多个查询条件

```java
{
"query": {
	"bool": { //组合查询
		"must": [{"term": {"hobby": "美女"}}], //must表示and的意思,其中term表示精确查询
		"should": [							//should:或的意思,也就是查询条件满足其一即可
            		{"term": {"hobby": "游戏"}}, 
 					{"term": {"hobby": "运动"}} 
        		  ],
		"must_not": [						//不等于的意思,查询条件中不要满足这个
						{"range" :{"birth_date":{"lt": "1990-06-30"}}} 
					],	
        "filter": [...],					//过滤
		"minimum_should_match": 1			//最小匹配个数
		}
	}
}
```

> 前缀匹配搜索-prefix

```java
{
"query": {
	"prefix": {
		"fullName": "倪"           //倪xxx
		}
	}
}
```

和term查询相似，前匹配搜索不是精确匹配，而是类似于SQL中的like ‘key%’

###  3.5 聚合查询

一般用于统计信息之类的

1. [Max Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/search-aggregations-metrics-max-aggregation.html):求最大值。基于文档的某个值计算该值在聚合文档中的均值
2. [Min Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/search-aggregations-metrics-min-aggregation.html):求最小值。同上
3. [Sum Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/search-aggregations-metrics-sum-aggregation.html):求和。同上
4. [Avg Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/search-aggregations-metrics-avg-aggregation.html):求平均数。同上

比如这里:[Max Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/search-aggregations-metrics-max-aggregation.html) : 查询价格最大值

```java
GET hrm/course/_search
{
  "aggs": {
    "price_max": { //起的别名
      "max": {		//关键词
        "field": "price"
      }
    }
  }
}
```

> 数量统计-"value_count"

```java
GET hrm/course/_search
{
  "aggs": {
    "id_count": {
      "value_count": {
        "field": "id"
      }
    }
  }
}
```

> 去重集合-distinct 

```java
GET hrm/course/_search
{
  "aggs": {
    "price_count": {
      "cardinality": {
        "field": "price"
      }
    },
    "id_count": {
      "cardinality": {  //去重
        "field": "id"
      }
    }
  }
}
```

>Top Hits-最高匹配

```java
#分组统计数量 
GET hrm/course/_search
{
  "aggs": {
    "price_terms": {
      "terms": {
        "field": "price",
        "size": 100
      },
      "aggs": {
        "tops": {
          "top_hits": { //统计每个价格的前10条数据
            "size": 10
          }
        }
      }
    }
  }
}
```

> 百分比排名聚合

```java
GET hrm/course/_search
{
  "aggs": {
    "price_percentiles": {
      "percentile_ranks": {  //统计price小于500和年龄小于600的文档的占比
        "field": "price",
        "values": [
          500,
          600
        ]
      }
    }
  }
}
```

### 3.6 分词与映射

查看索引类型的映射配置：GET {indexName}/_mapping/{typeName}

ES在没有配置Mapping的情况下新增文档，ES会尝试对字段类型进行猜测，并动态生成字段和类型的映射关系。

| JSON type                       | Field type |
| ------------------------------- | ---------- |
| Boolean: true or false          | "boolean"  |
| Whole number: 123               | "long"     |
| Floating point: 123.45          | "double"   |
| String, valid date:"2014-09-15" | "date"     |
| String: "foo bar"               | "string"   |

简单类型映射:

字段映射的常用属性配置列表:

| type             | 类型：基本数据类型，integer,long,date,boolean,keyword,text... |
| ---------------- | ------------------------------------------------------------ |
| enable           | 是否启用：默认为true。 false：不能索引、不能搜索过滤，仅在_source中存储 |
| boost            | 权重提升倍数：用于查询时加权计算最终的得分。                 |
| format           | 格式：一般用于指定日期格式，如 yyyy-MM-dd HH:mm:ss.SSS       |
| ignore_above     | 长度限制：长度大于该值的字符串将不会被索引和存储。           |
| ignore_malformed | 转换错误忽略：true代表当格式转换错误时，忽略该值，被忽略后不会被存储和索引。 |
| include_in_all   | 是否将该字段值组合到_all中。                                 |
| null_value       | 默认控制替换值。如空字符串替换为”NULL”，空数字替换为-1       |
| store            | 是否存储：默认为false。true意义不大，因为_source中已有数据   |
| index            | 索引模式：analyzed (索引并分词，text默认模式), not_analyzed (索引不分词，keyword默认模式)，no（不索引） |
| analyzer         | 索引分词器：索引创建时使用的分词器，如ik_smart,ik_max_word,standard |
| search_analyzer  | 搜索分词器：搜索该字段的值时，传入的查询内容的分词器。       |
| fields           | 多字段索引：当对该字段需要使用多种索引模式时使用。如：城市搜索New York  下面字段city既可以分词有可以不分词"city": {   "type": "text",   "analyzer": "ik_smart",   "fields": {      "raw": {         "type":  "keyword"       }   }}有些类型 有时候需要分词 有时候不需要分词city分词city.raw 不分词那么以后搜索过滤和排序就可以使用city.raw字段名match:{  “city”:“北京”}match:{  “city.raw”:“北京”} |

## 4.Java操作

### 4.1 原生API

导入maven依赖:

```xml-dtd
<dependency>
    <groupId>org.elasticsearch.client</groupId>
    <artifactId>transport</artifactId>
    <version>5.2.2</version>
</dependency>
<dependency>
    <groupId>org.apache.logging.log4j</groupId>
    <artifactId>log4j-api</artifactId>
    <version>2.7</version>
</dependency>
<dependency>
    <groupId>org.apache.logging.log4j</groupId>
    <artifactId>log4j-core</artifactId>
    <version>2.7</version>
</dependency>
```

 创建连接:

```java
Settings settings = Settings.builder().put("client.transport.sniff", true).build();
TransportClient client = new PreBuiltTransportClient(settings).addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("127.0.0.1"), 9300))
```

你可以设置client.transport.sniff为true来使客户端去嗅探整个集群的状态，把集群中其它机器的ip地址加到客户端中，这样做的好处是一般你不用手动设置集群里所有集群的ip到连接客户端，它会自动帮你添加，并且自动发现新加入集群的机器

> 创建文档索引

```java
IndexResponse response = client.prepareIndex("crm", "vip", "1").setSource(jsonDataText).get();
```

