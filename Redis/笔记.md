# 笔记

## 1.NoSQL概述

### 1.1 为什么要用NoSQL

我们现在正在处于一个大数据时代,而所谓的大数据就是指一般的数据库无法进行分析处理了!

从最开始的历史历程:

> 1、单机MySQL的年代!

这是最早的模型,用户从web端通过数据库访问层去跟数据库交互	

![image-20211009192643907](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211009192643907.png)

90年代,一个基本的网站访问量不会太大,单个数据库完全足够了!而在那个年代的时候,更多的去使用静态网页html~,所以服务器根本没有太大的压力!所以造成了我们的单机MySQL足够使用了。

思考一下？这种情况下：整个网站的瓶颈是什么？

1. 数据库数据量如果太大，一台机器放不下！MySQL存储是有限的;
2. 数据库的单表数据如果超过300万条,那么就一定要建立索引,而索引的底层使用B+Tree实现的,如果索引太大,那么也会造成一台机器内存放不下;
3. 数据库的访问量(读写混合)如果很大的话,那么这个性能也会收到影响,一个服务器承受不了;

以上这些都是在单机MySQL年代的弊端~

> 2、Memcached缓存+MySQL+垂直拆分(读写分离)

​	由于一台机器不够用的时候我们就会扩增服务器,但是我们要保证每台服务器的数据库都是一致的,所以这个时候就提出了一台机器是用于写内容的,另外的服务器用于查询,而修改内容后就会同步到另外的服务器上的数据库里去,所以我们来进行读的时候就可以去1和3里面读,写就去2号里写内容,这样的操作就叫读写分离;

​	网站80%的情况都是在读,每次都要去查询数据库的话就十分的麻烦!所以说我们希望减轻数据库的的压力,我们可以使用缓存来保证效率;缓存只要是解决读的问题,让每次用户如果访问的都是同一SQL,那么就把第一次的数据放到缓存里去,然后后面的再次访问那么就从缓存里获取!

![image-20211009194241743](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211009194241743.png)

发展过程:

1. 最早就是通过优化底层的数据结构、算法和索引的一些东西
2. 然后使用文件缓存的方式,由于文件缓存是IO操作的,并且大文件小文件都要使用IO操作也显得很麻烦
3. 推出了Memcached(当时最热门的技术)

> 3、分库分表+水平拆分+MySQL集群

​	首先从一张图可以看出:数据库服务器发生了改变,这里M:主节点 S:从节点,这里组合成了一个作战单元,也就是组合成一个小节点也是一个小集群,然后小集群就会有多个集群,而我们查询的话正常还是会走缓存,我们整个数据架构就会越来越大,而后台越来越安稳了,那么前台运行就会越来越流畅了,

​	集群怎么实现:比如说我查询一个用户信息,首先会去缓存里看看有没有,没有就去集群里查,如何查呢?首先比如这里3个集群,那么每个集群就会放置用户3分之1的用户数据,加起来就是一个完整的用户数据,而我们通过集群的一些机制就能找到数据在哪个地方,就能提高效率

![image-20211009212552436](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211009212552436.png)

本质:数据库(**读**,**写**),我们始终都在解决这个问题!

- 读:
  - 最开始单机MySQL的时候由于读写都在单机,由于并发量增大造成读的压力变大,我们使用缓存的方式来解决读的问题!

- 写:

  - 物理解决:

    - 早些年使用引擎是MyISM,有个缺点是表锁,那什么是表锁呢?比如说我在一个100万条数据的用户表中查一个数据,那么它就会把整个用户表进行锁起来,剩下的进程由于加锁了只能等待整个查询出来,就会十分的影响效率!高并发下就会出现严重的锁问题,那这个时候就开始转战Innodb!而Innodb是行锁,明显要比表锁效率高得多,行锁就是每次查询就只锁这一行而已!

  - 其他方式解决:

    - 慢慢的就开始使用分库分表来解决写的问题,比如用主从复制、集群的方式!比如说我们一个数据库里有很多表,那么我们就会将这些表按照比如订单、用户、支付不同的业务进行不同的数据库进行管理,如果一个User表里有大量的数据那么也会对字段进行拆分,后面做成了微服务!这个时候就用分库分表就来解决写的压力;

MySQL在那个年代推出了表分区,并没有让很多公司用起来!但是确实给那个年代带来了希望。同时推出了MySQL集群,很好满足了那个年代的所有需求!但是也没有解决大部分需求,因为我们现在数据已经没有那么单一了!

> 4、如今最近的年代

2010-2020十年之间，世界已经发生了翻天覆地的变化（定位，也是一种数据，音乐，热榜!)

像热榜这些动态实时刷新的数据用MySQL集群已经做不了,所以MySQL等关系型数据库就不够用了!因为数据量很多,变化很快!

MySQL有的使用它来存储一些比较大的文件:博客、图片等!就会造成数据库表很大,效率变低了!如果有一种数据库来专门处理这种数据那MySQL的压力就变得十分小(研究如何处理这些问题!),比如在大数据的IO压力下,表几乎无法更改(比如表有一亿条数据,现在增加一列,那么就要增加一亿条数据!);

> 目前一个基本的互联网项目!

从图中可知:用户并不是直接访问的数据库,而是首先访问的是企业防火墙,或者走一些路由网关,那么通过这个网关就要做一些事情就是负载均衡,然后通过负载均衡去搭上我们的服务器集群,通过DNS做一些轮询,通过服务器我们就去访问我们数据库的一些实例,还有我们的很多的服务器!   ![image-20211010090417721](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010090417721.png)
    

> 为什么要用NoSQL!

用户的个人信息、社交网络、地理位置。用户自己产生的数据(每天都是按TB来计算的)比如用户日志等等都是爆发性的增长,都是几千万条！关系型数据库已经无法满足需求了，已经达到瓶颈了！

这个时候我们就要使用NoSQL数据库了,NoSQL可以很好的处理以上的情况! 

### 1.2 什么是NoSQL

> NoSQL=Not Only SQL(不仅仅是SQL),泛指非关系型数据库的!

关系型数据库:就好比是表格,主要以行和列来记录的	

​	随着web2.0互联网的诞生!传统的关系型数据库很难对付web2.0时代!比如web2.0的产物:音乐、视频、个人社交的一些东西这些爆发性的东西，特别麻烦！尤其是超大规模的高并发社区！而社区在那个年代一般是指网站的站长之类的，而通常网站的流量得到增长的时候就会暴露出很多难以克服的问题，那么这个时候NoSQL在当今大数据环境下发展的十分迅速.Redis是发展最快的,并且是我们必须掌握的一个技术!

​	很多的数据类型用户的个人信息、社交网络、地理位置。这些数据类型的存储不需要一个固定的格式（就是行跟列，比如我们的地理位置不应该是一个固定的格式而是一个图谱，是动态发展的！），不需要的多余的操作就以横向扩展的（用多台机器实现比如集群）！

​	NoSQL比较典型的一种表现:Map<String,Object>,其中Obeject是能装万事万物的,这个是要以键值对的形式来控制

> NoSQL特点

1. 方便扩展(数据之间没有关系,因为KV键值对简单,很好扩展!),解耦

2. 大数据量高性能(Redis一秒写8万次,读11万条数据,NoSQL的缓存记录级的,就是一个记录可以实时的被刷新,是一种细粒度的缓存,性能会比较高!)

3. 数据类型是多样型的!(不需要事先设计数据库!就是键值对形式,我们可以随取随用!如果数据量十分大的表,很多话人就无法设计了)

   


传统的RDBMS和NoSQL的区别:

1. 传统的RDBMS
   - 结构化组织(行,列)
   - 基本的结构化查询SQL
   - 数据和关系都存在单独的表中
   - 操作语言,数据定义语言
   - 严格的一致性(ACID原则)
   - 基础的事务
   - ...
2. NoSQL
   - 不仅仅是数据
   - 没有固定的查询语言
   - 存储方式多种:键值对存储、列存储、文档存储、图形数据库（社交关系就可以用）
   - 最终一致性（是有误差的，但是只要最终结果一致性就可以了）
   - CAP定理 和 BASE理论,理论完用以实践的是"异地多活",就是保证整个服务器不会宕机,一个服务器挂了不会影响其他的服务器;
   - 保证三高问题:高性能（每秒写8万次..）、高可用和高可扩展
   - ...

最大的区别:关系型数据库我们需要学习SQL语言,非关系型数据库不是固定的,并且非关系数据库有存储方式有四种,而关系型数据库只有一种都放在单独的表中,表里就只有两个东西:行跟列

**真正在公司中的实践:NoSQL+RDBMS 一起使用才是最强的!**

我们学东西不是说学怎么安装,学习一下方法如何调用,那叫学习吗?问下自己学会这个东西了吗?真正的学习就应该是从头到尾认认真真学习完每一步,学习一个新东西就应该知道这个是什么意思?干什么的,它为什么诞生了,有什么优势..

> 了解:3V+3高

大数据时代的3V:主要是描述问题的

1. 海量Volume  :海量的数据
2. 多样Variety  :每种数据库都是不一样的,有聊天信息,聊天的图片信息,地图信息,位置信息这些都是不一样的
3. 实时Velocity  :实时性!比如直播的弹幕所接收到的一种实时性

大数据时代的3高:主要是对程序的要求

1. 高并发 :用户数量的增加和访问 
2. 高可拓 :可拓展性,随时水平拆分,可以搭建集群,一台机器不够了,可以扩展机器来解决
3. 高性能 :保证咱们用户体验和性能! 

   

### 1.3 阿里巴巴网页分析

首先我们进入阿里巴巴网站上可以看到:

![image-20211010103527312](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010103527312.png)

思考问题:这么多东西难道都是在一个数据库中的吗?

```bash
# 1. 商品信息
	名称、价格、商家信息
	关系性数据库就可以解决了！MySQL、Oracle（淘宝早些年去IOE了！-王坚，推荐文章：阿里云的这群疯子），淘宝内部的MySQL不是大家用的MySQL,因为MySQL底层是组件化的是可以进行热拔插, 所以人家根据自己的业务需求去进行改变MySQL!
	
# 2.商品的描述、评论（文字比较多）
	一般放在文档型数据库中,比如MongDB
	
# 3.图片
	分布式文件系统: FastDFS
	淘宝用自己的 TFS
	Gooale的 GFS
	Hadoop的 HDFS
	阿里云的 oss
	
# 4.商品的关键字(搜索)
	搜索引擎: solr、elasticsearch
	淘宝用的是 ISerach, -多隆(它一个人开发出来的)

# 5.商品热门的波段信息(秒杀)
	内存数据库: Redis、Tair、Memache...
	
# 6.商品的交易,外部的支付接口
	三方应用,比如支付宝接口、银行支付
```

所以说:一个简单的网页背后的技术一定不是那么的简单!

大型互联网应用问题:

1. 数据类型太多了!如果是一个电商网站那么就有这么多东西就要去处理!
2. 数据源繁多.经常重构!每个用户都可以上传自己的东西,在敏捷开发中我们就会接触实时重构这个东西!
3. 数据要改造,大面积改造! 如果改一个SQL,那么有关这个SQL的所有接口都要去改!

### 1.4 NoSQL的四大分类

**KV键值对**

不同公司的使用实现:

- 新浪:Redis
- 美团:Redis+Tair
- 阿里、百度:Redis+memecache

**文档型数据库(bson格式,和json一样只是传输功能不一样是二进制的json)**

- MongoDB
  - MongoDB是一个基于分布式文件存储的数据库,一般用于存储大量的文档,其本身是由C++编写的,主要用于处理大量的文档!
  - MongoDB是一个介于**关系型数据库**和**非关系型数据库**之间的产品!
  - MongoDB是非关系型数据库中功能**最丰富**,**最像**关系型数据库的!

- ConthDB

**列存储数据库**

- HBase
- 分布式文件系统  

**图关系数据库**

不是拿来存储图形的,是用于存储关系的,比如:朋友圈社交网络,广告推荐!

 ![image-20211010140906981](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010140906981.png)

- Neo4j
- InfoGrid



> 四者对比!

![image-20211010141235929](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010141235929.png)

## 2.Redis入门

### 2.1 概述

> Redis是什么?

Redis（**Re**mote **Di**ctionary **S**erver )，即远程字典服务!
	是一个开源的使用ANSI [C语言](https://baike.baidu.com/item/C语言)编写、支持网络、可基于内存亦可持久化的日志型、Key-Value[数据库](https://baike.baidu.com/item/数据库/103728)，并提供多种语言的API。

理解:首先Redis是C语言编写的,它支持网络所以我们可以通过HTTP协议来传输东西,它基于内存和持久化的,并且它还是以KV键值对的数据库,还有它提供了多种语言的API,也就是说除了Java语言以外我们可以用其他语言来调用Redis!
	redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步

**总之**:它是免费和开源的!是当下最热门的NoSQL技术之一,也被人们称之为结构化数据库!

> Redis能干嘛?

1. 内存存储、持久化，内存中是断电即失，所以持久化很重要（这里有两种机制:rdb,aof）
2. 效率高,可以用于高速缓存
3. 发布订阅系统-可以简单的做一些消息队列的功能
4. 地图信息分析
5. 计时器,计数器(浏览量计算!)
6. ...

> Redis特性

1. 开源
2. 支持多种数据类型
3. 支持持久化并提供多种语言API
4. 提供事务的控制 
5. 支持集群
6. ...



这是需要用到的网站:

1. redis官网:https://redis.io/
2. redis中文网:http://www.redis.cn/

下载地址:

![image-20211010144442496](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010144442496.png)

**注意**:Window在GitHub上下载(停更很久了)

Redis推荐的都是在Linux服务器上搭建的,所以最好是基于Linux来进行学习

### 2.2 安装

> 这是Window下的安装

1. 下载安装包:https://github.com/dmajkic/redis/releases

2. 下载并解压安装包:

   ![image-20211010145305029](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010145305029.png)

3. 解压后如以下目录:

   ![image-20211010145816869](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010145816869.png)

   从图中我们看出:

   1. `redis-cli.exe`跟`redis-server.exe`是客户端和服务端
   2. `redis-check-aof.exe`是检查aop持久化文件是不是对的
   3. `redis-benchmark.exe`是测试性能的

4. 开启Rdis,双击`redis-server.exe`即可

   ![image-20211010150200975](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010150200975.png)

   这里很重要!!!!!!!!!!!!

   ```
   如果直接点开redis-server.exe那么就会默认去找 /path/to/redis.conf这个配置文件,而我们window的配置文件是redis.window.conf!所以如果启动的话那么一切皆默认,我只能说!如果你想自定义的一些配置,那么都是不生效的!
   解决办法:
   	1. cd到redis当前目录下,然后执行命令 redis-server.exe redis.windows.conf,如果正常出现logo表示成功启动,如果不行,可能是你配置文件中某个位置单词没调整距离导致的!
   	2. 能正常启动,那么我们就建一个txt文件,把上述命令写进去,然后把后缀改为bat可执行文件,然后我们需要的时候就打开这个即可!!!!
   	3. 注意:如果用bat出现闪退,可能是你配置文件中有东西错了,所以先用cmd命令执行redis-server.exe redis.windows.conf,它会提示你的!
   ```

   最后执行成功就是这样:

   ![image-20211016103434035](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016103434035.png)

5. 使用redis客户端来连接redis

   ![image-20211010150601350](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010150601350.png)

   这里我们通过输入命令`ping`返回PONG表示已经连接成功了,然后我们输入set基本值 key-value,然后我们就可以通过key来获取value

> 注意:window下使用很简单,但是Redis官方推荐我们使用Linux去开发使用!

这是官方说明地址:http://www.redis.cn/topics/introduction.html

![image-20211010151343652](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010151343652.png)

> 这是Linux安装

由于我还没安装Linux进行学习,这里只是作为笔记,未来可回头查阅

1. 下载到本地资源:

![image-20211010152018242](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010152018242.png)

算了,这里附上链接,学完Linux后再来这里进行安装:

https://www.bilibili.com/video/BV1S54y1R7SB?p=9&spm_id_from=pageDriver

### 2.3 测试性能

在我们的文件夹中找到`redis-benchmark`,这是官方自带的性能测试工具,也是一个压力测试工具!

这是对应的命令参数:

![image-20211010153826476](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010153826476.png)

我们简单测试一下:

```bash
# 测试:100个并发连接,每个连接都是100000个请求
redis-benchmark -h localhost -p 6379 -c 100 -n 100000
```

![image-20211010171117790](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010171117790.png)

如何查看这些分析呢？

![image-20211010171633047](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010171633047.png)

### 2.4 基础的知识

> redis默认有16个数据库!

我们可以打开我们Redis配置文件中的`redis.windows-service.conf`用记事本打开:

![image-20211010172253285](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010172253285.png)

默认使用的是第0个,我们可以使用`select`进行切换数据库,并且可以使用`DBSIZE`:当前数据库的大小:

```bash
127.0.0.1:6379> ping # 检查当前客户端是否连接上服务器
PONG
127.0.0.1:6379> select 3 # 切换数据库
OK
127.0.0.1:6379[3]> DBSIZE # 这里查看3号数据库大小
(integer) 0
127.0.0.1:6379[3]> set bane Miao # set键值对
OK
127.0.0.1:6379[3]> DBSIZE # 再次查看就可以看到有值
(integer) 1
127.0.0.1:6379[3]> select 7
OK
127.0.0.1:6379[7]> get name
(nil)
127.0.0.1:6379[7]> select 3
OK
127.0.0.1:6379[3]> get bane
"Miao"
127.0.0.1:6379[3]>
```

不同的数据可以存不同的数据库;

> 基础命令:这些命令要记住,后面用Java实现的时候就是用这些命令命名的

1. 查看当前数据库所有的key:**keys *`**

   ```bash
   127.0.0.1:6379[3]> keys *
   1) "bane"
   ```

2. 清除当前数据库:**flushdb **

   ```bash
   127.0.0.1:6379[3]> flushdb 
   OK
   127.0.0.1:6379[3]> keys *
   (empty list or set)
   ```

3. 清除全部数据库:**flushAll**

   ```bash
   127.0.0.1:6379> select 1
   OK
   127.0.0.1:6379[1]> set name MiaoDaWei
   OK
   127.0.0.1:6379[1]> get name
   "MiaoDaWei"
   127.0.0.1:6379[1]> select 2
   OK
   127.0.0.1:6379[2]> set kk HowAreYou
   OK
   127.0.0.1:6379[2]> get kk
   "HowAreYou"
   127.0.0.1:6379[1]> flushAll # 清除全部数据库
   OK
   (0.52s)
   127.0.0.1:6379[1]> get name
   (nil)
   127.0.0.1:6379[1]> select 2
   OK
   127.0.0.1:6379[2]> get kk
   (nil)
   ```

4. 判断key是否存在:**exists**

   ```bash
   127.0.0.1:6379> set name miaowei
   OK
   127.0.0.1:6379> get name
   "miaowei"
   127.0.0.1:6379> exists name # 查询是否存在
   (integer) 1
   127.0.0.1:6379> exists name1 
   (integer) 0
   ```

5. 移动Key:**move**

   ```bash
   127.0.0.1:6379> move name # 移除key 后面没指定数据库表示移除,而如果表明了数据库db,那么这是移动key
   (integer) 1
   127.0.0.1:6379> get name
   (nil)
   ```

6. 给key设置过期时间:**expire**和查看剩余多少秒过期:**ttl**

   ```bash
   127.0.0.1:6379> keys * # 查看所有key
   1) "age"
   2) "name"
   127.0.0.1:6379> expire name 5 # 设置key为name的有效时间为5秒
   (integer) 1
   127.0.0.1:6379> ttl name
   (integer) 3 # 为整数表示剩余多少秒就过期了
   127.0.0.1:6379> ttl name # 查看key的剩余时间
   (integer) -2 # 为-2表示已经不存在了
   127.0.0.1:6379> get name # 获取key
   (nil)
   127.0.0.1:6379> ttl age
   (integer) -1 # 为-1表示永不过期
   ```

   

思考?为什么redis是6379!

这东西,百度就能搜到,似乎是作者为了一舞女还是前女友在九宫格键盘上打出她的名字,就是6379!

> Redis是单线程的!

​	首先我们得知道Redis很快的(这是从官方测试性能得出的),官方表示:Redis是基于内存操作,而CPU不是Redis性能瓶颈,我们知道多线程是跟CPU是挂钩的而Redis是基于内存的,所以说Redis瓶颈是**机器的内存**和**网络带宽**(因为要通过网络接收)决定的!

​	既然可以使用单线程实现就不会用多线程来做!所以就使用了单线程!

> Redis为什么单线程还这么快?

首先我们知道Redis是C语言写的,官方提供的数据为 10000+ 的QPS(每秒查询率),完全不比同样是使用key-value的Memecache差!所以我们就要思考它为什么就那么快?

误区:

1. 高性能的服务器一定是多线程的?不一定
2. 多线程(CPU上下文会切换!!)一定比单线程效率高?不一定
   - 注意:运行速度是:CPU>内存>硬盘

解答:

​	首先明白多线程不一定就会比单线程快!Redis是将所有的数据存放在内存中的,所以说使用单线程操作效率就是最高的!因为多线程之间会造成会产生CPU上下文切换,这个是耗时的操作,因为每次切换都会耗时1500-2000纳秒之间,而一旦多了就会耗时变长了!!!对于内存系统来说,如果没有上下文切换,效率就是最高的!

​	redis利用单个CPU的一块内存的数据的话,针对这块内存进行多次读写的话都是在一个CPU上完成的,多次读写都是在一个CPU上的,在内存情况下,这个就是最佳的方案!

## 3.数据类型

官方文档:

![image-20211010184714617](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010184714617.png)

全段翻译:

```
Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。
```

五大数据类型:

Redis-Key

2. String
3. List
4. Set
5. Hash
6. Zset

三种特殊数据类型

1. geospatial
2. hyperloglog
3. bitmaps 

### 3.1 Redis-Key

```bash
127.0.0.1:6379> keys * # 查看当前库所有的key
(empty list or set)
127.0.0.1:6379> set name miao # set key
OK
127.0.0.1:6379> keys *
1) "name"
127.0.0.1:6379> set age 1
OK
127.0.0.1:6379> keys *
1) "age"
2) "name"
127.0.0.1:6379> EXISTS name  # 查看key是否存在
(integer) 1 # 存在为1
127.0.0.1:6379> EXISTS name1 
(integer) 0 # 不存在为0
127.0.0.1:6379> move name 1 # 移动key到 1号数据库去,这里如果不写数据库index,那么是移除掉
(integer) 1 # 移动成功
127.0.0.1:6379> keys *
1) "age"
127.0.0.1:6379> set name wei
OK
127.0.0.1:6379> keys *
1) "age"
2) "name"
127.0.0.1:6379> get name
"wei"
127.0.0.1:6379> expire name 10 # 设置key的有效过期时间
(integer) 1
127.0.0.1:6379> ttl name # 查看key的剩余时间,单位是秒
(integer) 7
127.0.0.1:6379> ttl name
(integer) 4 # 整数,表示剩余秒数
127.0.0.1:6379> ttl name
(integer) -2 # 为-2,表示已经不存在了
127.0.0.1:6379> ttl age
(integer) -1 # 为-1,表示不会过期
127.0.0.1:6379> get name
(nil)
127.0.0.1:6379> clear # 用于清除在客户端窗口中使用过的记录
127.0.0.1:6379> type name # 用于查看key的数据类型
string
127.0.0.1:6379> type age
string
```

注意:我们在使用的时候,一般redis都会给出相应的提示,也就是所谓的帮助文档,比如我们写一个set,那么就会相应给出提示剩余该怎么写!

如果我们的命令太多记不住,那么我们可以去官网去查看:

![image-20211010193053218](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010193053218.png)

> 对应的命令一定要记住!因为后面用Jedis后,我们用Java去实现,那么对应的方法跟命令是一样的!

### 3.2 五种数据类型

#### 1. String(字符串类型)

基本命令:

```bash
####################################################################################################################
127.0.0.1:6379> set key1 v1 		# 设置值
OK
127.0.0.1:6379> get key1 			# 获得值
"v1"
127.0.0.1:6379> keys * 				# 查看所有key
1) "key1"
127.0.0.1:6379> exists key1			# 判断某个key是否存在
(integer) 1
127.0.0.1:6379> append key1 'hello' # 追加字符串,如果当前key不存在,就相当于set了一个key
(integer) 7
127.0.0.1:6379> get key1
"v1hello"
127.0.0.1:6379> strlen key1  		# 获取字符串的长度!
(integer) 7
127.0.0.1:6379> append key1 ',MiaoDaWei'
(integer) 17
127.0.0.1:6379> strlen key1
(integer) 17
127.0.0.1:6379> get key1
"v1hello,MiaoDaWei"
####################################################################################################################
# 步长,java中的i+=操作,可以用于网页浏览量等等!
# 注意:使用incr 只能是数字!
127.0.0.1:6379> set views 0  		# 设置初始浏览量为0
OK
127.0.0.1:6379> get views
"0"
127.0.0.1:6379> incr views 			# 自增1
(integer) 1
127.0.0.1:6379> incr views
(integer) 2
127.0.0.1:6379> get views
"2"
127.0.0.1:6379> decr views 			# 自减1
(integer) 1
127.0.0.1:6379> decr views
(integer) 0
127.0.0.1:6379> decr views
(integer) -1
127.0.0.1:6379> decr views
(integer) -2
127.0.0.1:6379> incrby views 12 	# 设置步长,指定增量!在原来基础+12
(integer) 10
127.0.0.1:6379> get views
"10"
127.0.0.1:6379> decrby views 10 	# 设置步长,指定减量!在原来基础上-10
(integer) 0
127.0.0.1:6379> get views
"0"
####################################################################################################################
# 字符串范围操作 range
127.0.0.1:6379> set name "hello,MiaoDaWei" 	# 设置key
OK
127.0.0.1:6379> get name
"hello,MiaoDaWei"
127.0.0.1:6379> getrange name 0 4 			# 截取key的范围,下标从0开始,这是闭区间[0,4]
"hello"
127.0.0.1:6379> get name
"hello,MiaoDaWei"
127.0.0.1:6379> getrange name 0 -1 			# 截取key的范围,结束下标为-1,表示截取整个字符串
"hello,MiaoDaWei"
替换!类似于Java中的subString跟replace:
127.0.0.1:6379> set name2 abcdefg
OK
127.0.0.1:6379> setrange name2 1 xx			# 替换指定位置的开始的字符串!
(integer) 7
127.0.0.1:6379> get name2
"axxdefg"
####################################################################################################################
# setex跟setnx,用于判断当前值是否存在,存在就设值,
# setex(set with expire) :设置过期时间
# setnx(set if not exist):不存在就设值 (在分布式锁中会常常使用!可以保证值的存在)
127.0.0.1:6379> setex key3 30 'hello'		#设置key3的值为hello,并且30秒后过期,注意:key3如果存在就会替换,不存在就会创建,总之只要setex就会有值
OK
127.0.0.1:6379> ttl key3
(integer) 22
127.0.0.1:6379> get key3
"hello"
127.0.0.1:6379> setnx mykey "redis" 		# 如果mykey不存在,就创建mykey
(integer) 1
127.0.0.1:6379> keys *
1) "name"
2) "mykey"
3) "name2"
127.0.0.1:6379> ttl key3
(integer) -2
127.0.0.1:6379> setnx mykey "MongoDB"		# 如果mykey存在,那么创建失败!
(integer) 0
127.0.0.1:6379> get mykey
"redis"
####################################################################################################################
# mset:批量set值
# mget:批量获取值
127.0.0.1:6379>  mset k1 v1 k2 v2 k3 v3		# 批量设置值,以空格隔开,前面是key 后面是value
OK
127.0.0.1:6379> keys *
1) "k1"
2) "k3"
3) "k2"
127.0.0.1:6379> mset k4 v4 k4 v5			# 如果在批量set的时候,key重复了那么就会以后面创建的为准,也就是后面替换前面的
OK
127.0.0.1:6379> get k4
"v5"
127.0.0.1:6379> mget k1 k2 k3 k4 k5			# 批量获取key,都是key并且以空格隔开,有值就获取没值获取不到
1) "v1"
2) "v2"
3) "v3"
4) "v5"
5) (nil)
127.0.0.1:6379>  mset k1 v1 k5 v5
OK
127.0.0.1:6379> msetnx k1 v1 k6 v6			# 批量操作set值,这里是原子性,但凡在批量操作中出现已经存在的key那么就会操作失败返回0,要么同时成功要么同时失败!
(integer) 0

# 对象
set user:1 {name:zhangsan,age:3}  # 设置一个user:1对象 值为json字符来保存一个对象!

# 这里key是一个巧妙的设计:	user:{id}:{filed},如此设计在Redis中是完全是OK的!

127.0.0.1:6379> mset user:1:name zhangsan user:1:age 2
OK
127.0.0.1:6379> mget user:1:name user:1:age
1) "zhangsan"
2) "2"
####################################################################################################################
getset # 先get然后再set
127.0.0.1:6379> getset db redis				# 先获取db在set值,也就是说先获取,不管是否存在都不影响后面的set,如果不存在值就返回null.存在就设置新的值
(nil)
127.0.0.1:6379> get db
"redis"
127.0.0.1:6379> getset db mongodb			# 获取存在的key,然后set值
"redis"
127.0.0.1:6379> get db
"mongodb"
```

其实本身数据结构都是相通的,后面我们就要使用`jedis`中在java中把命令变成一个个的方法!

String类型的使用场景:value除了是我们的字符串还可以是我们的数字!

- 比如计数器
- 统计多单位的数量
- 对象缓存存储!
- 粉丝数!

#### 2. List(列表)

这是一个基本的数据类型->列表

![image-20211010205513293](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211010205513293.png)

在redis里面,我们可以把list玩成:栈、队列、阻塞队列！

常用命令：

**注意**：所有的list命令都是以`l`开头的,并且不区分大小写命令:

```bash
####################################################################################################################
# 这里list只是key的名称而已
127.0.0.1:6379> lpush list one			# 将一个值或多个值插入到列表头部(左)
(integer) 1
127.0.0.1:6379> lpush list two
(integer) 2
127.0.0.1:6379> lpush list three
(integer) 3
127.0.0.1:6379> lrange list 0 -1
1) "three"
2) "two"
3) "one"
127.0.0.1:6379> lrange list 0 1
1) "three"
2) "two"
127.0.0.1:6379> rpush list righr		# 将一个值或多个值插入到列表尾部(右)
(integer) 4
127.0.0.1:6379> lrange list 0 -1
1) "three"
2) "two"
3) "one"
4) "righr"
# 理解:可以想象成一个平面图,使用lpush就会往左边插入,后面的值插入就会让前一个值往右移一位!,rpush反之亦然
####################################################################################################################
LPOP:# 往左边移除
RPOP:# 往右边移除 ,其实可以理解数据结构中的队列
127.0.0.1:6379> lrange list 0 -1
1) "three"
2) "two"
3) "one"
4) "righr"
127.0.0.1:6379> lpop list				# 移除列表的第一个元素
"three"
127.0.0.1:6379> rpop list				# 移除列表的最后一个元素
"righr"
127.0.0.1:6379> lrange list 0 -1
1) "two"
2) "one"
####################################################################################################################
# 理解为数组的话我们可以通过下标来获取值
lindex :# 获取某一个列表的下标值
127.0.0.1:6379> lrange list 0 -1		# 获取列表全部元素
1) "two"
2) "one"
127.0.0.1:6379> lindex list 0			# 获取下标为0的元素
"two"
127.0.0.1:6379> lindex list 1
"one"
127.0.0.1:6379> lindex list 2			# 因为下标没有元素所以返回null
(nil)
####################################################################################################################
Llen:# 获取列表的长度
127.0.0.1:6379> lpush list one two three # 往列表里插入多个值
(integer) 3
127.0.0.1:6379> llen list				 # 获取列表的长度,也就是有多少个元素
(integer) 3
####################################################################################################################
lrem:# 移除指定的值!
127.0.0.1:6379> lpush list three		# 往列表头部插入一个元素
(integer) 4
127.0.0.1:6379> lrange list 0 -1
1) "three"
2) "three"
3) "two"
4) "one"
127.0.0.1:6379> lrem list 1 one			# 移除列表中指定个数的value,精确匹配!注意:如果指定个数比实际个数多了,也不会影响!
(integer) 1
127.0.0.1:6379> lrange list 0 -1
1) "three"
2) "three"
3) "two"
127.0.0.1:6379> lrem list 2 three
(integer) 2
127.0.0.1:6379> lrange list 0 -1
1) "two"
####################################################################################################################
ltrim: # 截断某一部分的数据
127.0.0.1:6379> rpush list "hello"
(integer) 1
127.0.0.1:6379> rpush list "hello1"
(integer) 2
127.0.0.1:6379> rpush list "hello2"
(integer) 3
127.0.0.1:6379> rpush list "hello3"
(integer) 4
127.0.0.1:6379> lrange list 0 -1
1) "hello"
2) "hello1"
3) "hello2"
4) "hello3"
127.0.0.1:6379> ltrim list 1 2			# 通过下标截取指定的长度!这个list截取后就已经发生了改变,截断后只剩下截取的元素!
OK
127.0.0.1:6379> lrange list 0 -1
1) "hello1"
2) "hello2"
####################################################################################################################
rpoplpush: # 移除列表最后一个元素并将该元素添加到新的列表头部并返回,这是一个组合命令
127.0.0.1:6379> rpush mulist "hello"
(integer) 1
127.0.0.1:6379> rpush mulist "hello1"
(integer) 2
127.0.0.1:6379> rpush mulist "hello2"
(integer) 3
127.0.0.1:6379> lrange mulist 0 -1
1) "hello"
2) "hello1"
3) "hello2"
127.0.0.1:6379> rpoplpush mulist list		# 将当前列表尾部的元素移除,然后将移除的元素push到另一个列表的头部	
"hello2"
127.0.0.1:6379> lrange mulist 0 -1			# 查看原来的列表
1) "hello"
2) "hello1"
127.0.0.1:6379> lrange list 0 -1			# 查看目标列表中,确实存在改值!
1) "hello2"
####################################################################################################################
lset: # 往存在的列表中对指定下标的值进行替换为另外一个值进行更新操作
127.0.0.1:6379> exists list					# 判断这个列表是否存在
(integer) 0
127.0.0.1:6379> lset list 0 miao			# 更新不存在的列表就会报错
(error) ERR no such key
127.0.0.1:6379> lpush list value1
(integer) 1
127.0.0.1:6379> lrange list 0 0
1) "value1"
127.0.0.1:6379> lset list 0 miao			# 如果存在就会更新当前下标的值
OK
127.0.0.1:6379> lrange list 0 0
1) "miao"
127.0.0.1:6379> lset list 1 wei				# 更新不存在的下标就会提示报错!
(error) ERR index out of range
####################################################################################################################
linsert: # 将某个具体的value插入到列表中具体的某个元素的	前面/后面
127.0.0.1:6379> rpush mylist "hello"
(integer) 1
127.0.0.1:6379>  rpush mylist "hello1"
(integer) 2
127.0.0.1:6379> linsert mylist before "world" "other"		# 在列表中指定的元素前面插入value
(integer) -1
127.0.0.1:6379> linsert mylist before "hello1" "other"
(integer) 3
127.0.0.1:6379> lrange mylist 0 -1
1) "hello"
2) "other"
3) "hello1"
127.0.0.1:6379> linsert mylist after "hello1" "after"		# 在列表中指定的元素后面插入value
(integer) 4
127.0.0.1:6379> lrange mylist 0 -1
1) "hello"
2) "other"
3) "hello1"
4) "after"
```

> 小结

1. 它实际上是一个链表的结构,我们可以在指定的节点的前后进行操作 before Node after,left right都可以插入值
2. 在push的时候如果key不存在就会创建新的链表!
3. 如果key存在,就会新增内容
4. 如果移除了所有的值就会变成一个空链表,而一个空链表就代表不存在!
5. 在两边插入或者改动值,效率是最高的!如果改动中间元素效率相对会低!



一般应用场景为消息排队、消息队列;

我们可以把列表当成队列使用:`Lpush Rpop` 先进先出

​			   堆栈使用:`Lpush Lpop` 先进后出	

这是列表最大的**优势**:既可以当成栈也可以当成队列!

#### 3. Set(集合)

这个Set也是一个集合类型可以存储多个值,但是**不能重复**!

基本命令:

**Set数据类型都是以S开头的命令,多以member表示元素**

```bash
####################################################################################################################
sadd:# 添加元素
smembers: # 查看所有元素
sismember: # 判断是否包含指定元素

127.0.0.1:6379> sadd myset "hello"				# set集合中添加元素
(integer) 1
127.0.0.1:6379> sadd myset "hello1"
(integer) 1
127.0.0.1:6379> sadd myset "hello1"				# 重复添加就会失败!
(integer) 0
127.0.0.1:6379> smembers myset					# 查看指定set的所有值
1) "hello"
2) "hello1"
127.0.0.1:6379> sismember myset "hello"			# 判断某个元素是否在set中
(integer) 1
127.0.0.1:6379> sismember myset "world"
(integer) 0
####################################################################################################################
127.0.0.1:6379> scard myset						# 获取set集合中内容元素个数
(integer) 2
127.0.0.1:6379> sadd myset "MiaoDaWei"
(integer) 1
127.0.0.1:6379> scard myset
(integer) 3
####################################################################################################################
srem: # 移除set集合中指定的一个元素

127.0.0.1:6379> scard myset
(integer) 3
127.0.0.1:6379> srem myset hello				# 移除set集合中指定元素
(integer) 1
127.0.0.1:6379> scard myset
(integer) 2
127.0.0.1:6379> smembers myset					# 查看set中全部元素
1) "hello1"
2) "MiaoDaWei"
####################################################################################################################
set 无序不重复集合。抽随机！
srandmember: # 随机抽取元素,可以指定抽取个数-由于数据量少容易出现偶然性

127.0.0.1:6379> smembers myset
1) "hello"
2) "hello1"
3) "MiaoDaWei"
127.0.0.1:6379> srandmember myset				# 随机抽选一个元素
"MiaoDaWei"
127.0.0.1:6379>  srandmember myset
"hello1"
127.0.0.1:6379>  srandmember myset 2			# 随机抽选出指定个数的元素
1) "hello1"
2) "MiaoDaWei"
127.0.0.1:6379>  srandmember myset 2
1) "hello1"
2) "MiaoDaWei"
127.0.0.1:6379>  srandmember myset 2
1) "hello"
2) "MiaoDaWei"
####################################################################################################################
# 删除随机的元素

127.0.0.1:6379> smembers myset
1) "hello"
2) "hello1"
3) "MiaoDaWei"
127.0.0.1:6379> spop myset						# 移除Set集合中随机的一个元素,pop:弹出
"hello"
127.0.0.1:6379> spop myset 2					# 移除Set集合中指定个数的随机元素
1) "hello1"
2) "MiaoDaWei"
127.0.0.1:6379> smembers myset
(empty list or set)
####################################################################################################################
# 将一个指定的值移动到另外一个Set集合中!

127.0.0.1:6379> sadd myset "hello"
(integer) 1
127.0.0.1:6379> sadd myset "hello2"
(integer) 1
127.0.0.1:6379> sadd myset "MiaoDaWei"
(integer) 1
127.0.0.1:6379> sadd myset2 "set2"
(integer) 1
127.0.0.1:6379> smembers myset
1) "hello"
2) "hello2"
3) "MiaoDaWei"
127.0.0.1:6379> smembers myset2
1) "set2"
127.0.0.1:6379> smove myset myset2 "MiaoDaWei"			# 将一个指定的值从当前Set集合移动到另外一个Set集合中去!
(integer) 1
127.0.0.1:6379> smembers myset
1) "hello"
2) "hello2"
127.0.0.1:6379> smembers myset2
1) "set2"
2) "MiaoDaWei"
####################################################################################################################
我们的微博或者哔哩哔哩都会发现有共同关注:
数学集合类:
	-  差集
	-  交集
	-  并集	

127.0.0.1:6379> sadd key1 a
(integer) 1
127.0.0.1:6379> sadd key1 b
(integer) 1
127.0.0.1:6379> sadd key1 c
(integer) 1
127.0.0.1:6379> sadd key2 c
(integer) 1
127.0.0.1:6379> sadd key2 d
(integer) 1
127.0.0.1:6379> sadd key2 e
(integer) 1
127.0.0.1:6379> smembers key1
1) "c"
2) "b"
3) "a"
127.0.0.1:6379> smembers key2
1) "d"
2) "c"
3) "e"
127.0.0.1:6379> sdiff key1 key2						# 差集:以第一个Set集合为参照物,然后跟二个Set集合进行比对,然后获取到第一个Set集合中的差集
1) "a"
2) "b"
127.0.0.1:6379> sinter key1 key2					# 交集:获取第一个Set集合与第二个Set集合共同的元素->共同好友就是这样实现的!
1) "c"
127.0.0.1:6379> sunion key1 key2					# 并集:获取第一个Set集合和第二个Set集合中不重复的元素!
1) "a"
2) "b"
3) "c"
4) "d"
5) "e"
####################################################################################################################
```

这里交集的应用场景:

​	比如微博,对于A用户来说将所有关注的人放在一个Set集合中,将它的粉丝也放在一个集合中!共同关注:A用户和B用户的共同关注就可以实现了,我们把这两个关注的人做一个交集,那么就出来了,类似的也有很多,比如共同爱好之类的,

这里提一个概念:二度好友,推荐好友!-->六度分隔理论!



#### 4. Hash(哈希)

​	可以把这想象成一个Map集合!首先我们在Map集合中存储的数据结构是`Key-Value`,而我们则是将value改成`Key-Map`集合!也就是`Key-<key-value>`结构,存储的话也是键值对,只不过这个时候这个值是一个map集合!

​	本质上跟String类型没有太大区别!还是简单的一个key-value!只不过这个值是有两个值进行组合的!

**所有的Hash都是以H开头的命令**

基本命令:

```bash
####################################################################################################################
127.0.0.1:6379> hset myhash filed1 miaowei				# 设置一个key value为一个map
(integer) 1
127.0.0.1:6379> hget myhash filed1						# 获取一个字段值
"miaowei"
127.0.0.1:6379> hset myhash filed2 wei
(integer) 1
127.0.0.1:6379> hget myhash filed2
"wei"
127.0.0.1:6379> hmset myhash filed3 hello filed2 world	 # set 多个key-value
OK
127.0.0.1:6379> hmget myhash filed1 filed2 filed3		# 获取多个字段值
1) "miaowei"
2) "world"
3) "hello"
127.0.0.1:6379> hgetall myhash							# 获取全部的数据,展示的数据以键跟值进行展示
1) "filed1"
2) "miaowei"
3) "filed2"
4) "world"
5) "filed3"
6) "hello"
127.0.0.1:6379> hdel myhash filed1						# 删除hash指定的key字段!对应的value值也就没有了!这里可以批量删除!
(integer) 1
127.0.0.1:6379> hgetall myhash
1) "filed2"
2) "world"
3) "filed3"
4) "hello"
####################################################################################################################
hlen: # 获取hash中有多少个值

127.0.0.1:6379> hgetall myhash
1) "filed2"
2) "world"
3) "filed3"
4) "hello"
127.0.0.1:6379> hlen myhash								# 获取hash表的字段数量
(integer) 2
####################################################################################################################
hexists: # 判断hash中是否有指定的key

127.0.0.1:6379> hgetall myhash
1) "filed2"
2) "world"
3) "filed3"
4) "hello"
127.0.0.1:6379> hexists myhash filed2					# 判断hash中指定字段是否存在!
(integer) 1
127.0.0.1:6379> hexists myhash filed4
(integer) 0
####################################################################################################################
# 只获得所有所有的filed

127.0.0.1:6379> hkeys myhash							# 获取所有的字段
1) "filed2"
2) "filed3"
127.0.0.1:6379> hvals myhash							# 获取所有的value
1) "world"
2) "hello"
####################################################################################################################
incr decr  # 自增

127.0.0.1:6379> hset myhash field3 5					# 指定初始数量
(integer) 1
127.0.0.1:6379> hincrby myhash field3 1					# 指定增量
(integer) 6
127.0.0.1:6379> hincrby myhash field3 -1				# 指定增量为负,就表示为decrby自减
(integer) 5
127.0.0.1:6379> hsetnx myhash field3 hello				# 如果存在则不能设置
(integer) 0
127.0.0.1:6379> hsetnx myhash field4 hello
(integer) 1

注:可以应用在分布式锁中!
```

应用场景:

​	hash变更的数据,我们可以把一个用户user当成一个键,而User里面有很多name和age,那么我们可以把name和age当成我们的值!我们可以把对象作为键,然后里面存放着就是具体的字段和字段的值,这样我们就能通过一个对象就能获取对应的字段!

​	Hash尤其是可以用一些用户信息的保存!特别是经常变动的信息!hash更适合于**对象的存储**!String更加适合字符串的存储!

类似:

```bash
127.0.0.1:6379> hmset user:1 name miaowei age 21
OK
127.0.0.1:6379> hget user:1 name
"miaowei"
127.0.0.1:6379> hget user:1 age
"21"
```

#### 5. Zset(有序集合)

​	在set的基础上增加了一个值,之前是set k1 v1,现在是zset k1 score1 v1,中间多了一个值用于排序!

​	本身跟Set并没有任何区别,这个也是不能重复的,但是这个跟Set明显区别就是多了一个可以用于排序的值!

常用命令(**同样是以Z开头的**):

```bash
####################################################################################################################
127.0.0.1:6379> zadd myzset 1 one					# 添加一个值
(integer) 1
127.0.0.1:6379> zadd myzset 3 three 2 two			# 批量添加多个值
(integer) 2
127.0.0.1:6379> zrange myzset 0 -1					# 查看全部值
1) "one"
2) "two"
3) "three"
####################################################################################################################
# 排序如何实现?

127.0.0.1:6379> zadd salary 2500 xiaohong							# 添加三个用户
(integer) 1
127.0.0.1:6379> zadd salary 5000 zhangsan
(integer) 1
127.0.0.1:6379> zadd salary 500 MiaoDaWei
(integer) 1
127.0.0.1:6379> zrange salary 0 -1									# 查看全部用户,默认是按照中间排序的值进行升序排列
1) "MiaoDaWei"
2) "xiaohong"
3) "zhangsan"
127.0.0.1:6379> zrangebyscore salary -inf +inf						# 查看全部用户通过排序值,并且负无穷到正无穷,也就是从小到大排序
1) "MiaoDaWei"
2) "xiaohong"
3) "zhangsan"
127.0.0.1:6379> zrangebyscore salary 0 -1							# 获取失败,最小值不能超过最大值
(empty list or set)
127.0.0.1:6379> zrangebyscore salary 0 5
(empty list or set)
127.0.0.1:6379> zrangebyscore salary 0 1000							# 这里就是获取1000以下的值
1) "MiaoDaWei"
127.0.0.1:6379> zrangebyscore salary -inf +inf withscores			# 显示最小到最大的用户并且附带成绩,就是有序集合的值
1) "MiaoDaWei"
2) "500"
3) "xiaohong"
4) "2500"
5) "zhangsan"
6) "5000"
127.0.0.1:6379> zrevrange salary 0 -1								# 从大到小进行排序!反转排序
1) "zhangsan"
2) "MiaoDaWei"
####################################################################################################################
# 移除zset中的元素

127.0.0.1:6379> zrange salary 0 -1
1) "MiaoDaWei"
2) "xiaohong"
3) "zhangsan"
127.0.0.1:6379> zrem salary xiaohong								# 移除有序集合中的指定元素
(integer) 1
127.0.0.1:6379> zrange salary 0 -1
1) "MiaoDaWei"
2) "zhangsan"
127.0.0.1:6379> zrange salary 0 -1									# 获取有序集合中的个数
1) "MiaoDaWei"
2) "zhangsan"
127.0.0.1:6379> zcard salary
(integer) 2
####################################################################################################################
# 按区间来计算

127.0.0.1:6379> zadd myset 1 hello 2 world 3 miaowei
(integer) 3
127.0.0.1:6379> zcount myset 1 3									# 获取指定区间的成员数量,包括1到3
(integer) 3
127.0.0.1:6379> zcount myset 1 2
(integer) 2
```

应用场景:

​	Set能做的事情它也能做,Set是无序的,而Zset是有序的,所以我们一般可以存储班级成绩表,工资表排序!我们还可以让一些消息带有权重,比如我们把普通消息设置为1,重要消息设置为2,所以我们可以带权重进行判断!

​	比如我们在B站或者微博热搜排行榜,我们就可以用Zset集合来做,把所有的播放量或者评分放在有序集合中,然后进行遍历,然后每一分钟进行刷新!然后还可以取Top N测试!

### 3.3 三种特殊数据类型

#### 1. geospatial(地理位置)

这种数据类型就可以实现朋友的定位、附近的人、打车的距离计算!

这种特殊类型再很早Redis3.2版本就推出了!这个功能就可以推算出地理位置的信息,两地之间的距离,方圆几里的人!

我们使用这种数据类型需要提供经纬度,所以我们可以查询一些测试数据:http://www.jsons.cn/lngcode/

只有六个命令使用(我们可以把`geospatial`只读前三个单词就是`geo`,然后我们再去看命令,是不是清晰多了):

![image-20211012211029794](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211012211029794.png)

官方文档:https://www.redis.net.cn/order/3685.htm

> geoadd

```bash
# geoadd 添加地理位置
# 规则:南北两极不能直接添加,我们一般会下载城市数据,直接通过Java程序一次性导入!
# 参数 geoadd 中国:城市 经度、纬度 名称
127.0.0.1:6379> geoadd china:city 116.40 39.90 beijing			# 添加城市经纬度
(integer) 1
127.0.0.1:6379> geoadd china:city 121.47 31.23 shanghai
(integer) 1
127.0.0.1:6379> geoadd china:city 106.50 29.53 chongqing 114.05 22.52 sehnzhen
(integer) 2
127.0.0.1:6379> geoadd china:city 120.16 30.21 hangzhou
(integer) 1
127.0.0.1:6379> geoadd china:city 108.96 34.26 xian
(integer) 1
# 这是错误示范,超出了范围:
#有效的经度从-180度到180度。
#有效的纬度从-85.05112878度到85.05112878度。
#当坐标位置超出上述指定范围时，该命令将会返回一个错误
127.0.0.1:6379> geoadd china:city 39.90 116.40  beijing
(error) ERR invalid longitude,latitude pair 39.900000,116.400000
```

> geopos

```bash
# geopos 获取指定城市的经纬度
# 获得当前定位: 一定是一个坐标值
127.0.0.1:6379> geopos china:city beijing			# 获取指定的城市经度和纬度
1) 1) "116.39999896287918"
   2) "39.900000091670925"
127.0.0.1:6379> geopos china:city chongqing hangzhou
1) 1) "106.49999767541885"
   2) "29.529999579006592"
2) 1) "120.16000002622604"
   2) "30.209999363307084"
```

> geoDist

```bash
# geoDist 返回两个给定位置之间的距离
# 如果两个位置之间的其中一个不存在， 那么命令返回空值。
# m 表示单位为米。
# km 表示单位为千米。
# mi 表示单位为英里。
# ft 表示单位为英尺。
# 如果用户没有显式地指定单位参数， 那么 GEODIST 默认使用米作为单位。
127.0.0.1:6379> geodist china:city beijing shanghai				# 查看上海到到北京的直线距离 默认单位为m
"1067378.7564"
127.0.0.1:6379> geodist china:city beijing shanghai km			# 查看上海到北京的直线距离 ,指定单位为km
"1067.3788"
127.0.0.1:6379> geodist china:city chongqing beijing km
"1464.0708"

```

> geoRadius

如何去做附近的人?(首先获得附近的人的地址,进行定位!然后不定时刷新定位把附近的定位加入到集合中!),通过半径进行查询!

注意:所有数据都应该录入key中,如:china:city,才会让结果更加清晰!

```bash
# geoRadius 以给定的经纬度为中心,找出某一半径的元素
127.0.0.1:6379> georadius china:city 110 30 1000 km											# 获取指定key中以 110 30 经纬度为中心,寻找方圆半径1000 km的城市
1) "chongqing"
2) "xian"
3) "sehnzhen"
4) "hangzhou"
127.0.0.1:6379> georadius china:city 110 30 500 km
1) "chongqing"
2) "xian"
127.0.0.1:6379> georadius china:city 110 30 500 km withcoord								# 显示他人的定位信息
1) 1) "chongqing"
   2) 1) "106.49999767541885"
      2) "29.529999579006592"
2) 1) "xian"
   2) 1) "108.96000176668167"
      2) "34.2599996441893"
127.0.0.1:6379> georadius china:city 110 30 500 km withdist									# 显示到中心距离的位置,以11 30 为中心!,就是显示这俩的直线距离
1) 1) "chongqing"
   2) "341.9374"
2) 1) "xian"
   2) "483.8340"
127.0.0.1:6379> georadius china:city 110 30 500 km withdist withcoord count 1				# 筛选出指定指定个数的结果
1) 1) "chongqing"
   2) "341.9374"
   3) 1) "106.49999767541885"
      2) "29.529999579006592"
127.0.0.1:6379> georadius china:city 110 30 500 km withdist withcoord count 1 desc			# 筛选出指定个数的结果并按照倒序排列
1) 1) "xian"
   2) "483.8340"
   3) 1) "108.96000176668167"
      2) "34.2599996441893"

```

> georadiusBymember

上面是根据经纬度去找周围人的经纬度,而这个是根据城市与城市之间的定位,我们也可以用做小区与小区之间的定位!

```bash
# georadiusBymember 找出位于指定范围内的元素,中心点是由给定的位置元素决定
127.0.0.1:6379> georadiusBymember china:city beijing 1000 km								# 查找指定城市为中心为半径1000km以内的城市
1) "beijing"
2) "xian"
127.0.0.1:6379>  georadiusBymember china:city shanghai 400 km
1) "hangzhou"
2) "shanghai"
```

> geohash

```bash
# geohash 返回一个或多个位置元素的Geohash表示-了解即可
# 该命令将返回11个字符的Geohash字符串!
# 将经纬度转换为字符串,这个字符串是11位的hash
# 如果两个字符串长得越像越接近,那么则两者距离就越近!
127.0.0.1:6379> geohash china:city beijing chongqing										# 将二维的经纬度转换为一维的字符串
1) "wx4fbxxfke0"
2) "wm5xzrybty0"
```

> GEO 底层的实现原理!

geo底层的实现原理其实就是Zset有序集合!geo这里面每个地址都可以标一个序号!所以说我们可以Zset命令来操作geo!

来看一下如何去实现:

```bash
# 我们看了geo命令是没有删除的,那么再来看这个
# 所以说基本的数据类型是很重要的,像这种特殊数据类型就是基于底层封装的
127.0.0.1:6379> zrange china:city 0 -1					# 查看全部的元素
1) "chongqing"
2) "xian"
3) "sehnzhen"
4) "hangzhou"
5) "shanghai"
6) "beijing"
127.0.0.1:6379> zrem china:city beijing					# 移除指定的元素
(integer) 1
127.0.0.1:6379> zrange china:city 0 -1
1) "chongqing"
2) "xian"
3) "sehnzhen"
4) "hangzhou"
5) "shanghai"
```



#### 2. Hyperloglog(基数统计)

> 什么是基数?

比如说这里有两个集合:A{1,2,3,4,4},B:{1,2,3,4},而基数呢?说直白点就是两个集合共同不重复的元素,这里个数为:four:,所以我们要去两个集合中找不重复的元素,如果数量特别大,是可以有误差的!

> 简介

Redis 2.8.9版本就更新了Hyperloglog这种数据结构!所以它有自己的算法。

Redis Hyperloglog 是用于基数统计的算法！

优点:占用的内存是固定的,比如我们这里存储2^64次方-long类型的最大值,然后我们存储不同的元素的基数,只需要费12kb的内存!

**如果从内存的角度来讲比较的话Hyperloglog就是首选!**

​	比如：统计一个网页的UV(UV:页面访问量),我们这里一个人访问一个网站多次,但是还是要算作一个人!如果不是这样那么我们统计出来就是错误的!

​	传统的方式:我们使用Set集合保存用户的id,如果出现一样的id那么就会被干掉!不允许重复,就可以统计Set集合中的元素数量,作为标准判断,当然也有可能存在误差,比如在并发的时候,但是有的用户id比较麻烦,使用的uuid或者分布式id,这种呢就会导致id特别特别的长,那么这种方式保存大量的用户id就会特别麻烦!占内存!消耗了大量的存储空间去存储,而我们的目的并不是去保存这个的用户id,而是去计数!



官方:在官方文档里提出有0.81%的错误率,统计UV任务是可以忽略不计的!

所有的hyperloglog的基本命令都是以PF开头的:

```bash
# 测试使用:
127.0.0.1:6379> pfadd mykey a b c d e f g h i j				# 创建第一组元素
(integer) 1
127.0.0.1:6379> pfcount mykey								# 统计第一组元素基数数量(重复的话就会被过滤掉,统计的都是不能重复的元素)
(integer) 10
127.0.0.1:6379> pfadd mykey2 i j z x c v b n m
(integer) 1
127.0.0.1:6379> pfcount mykey2
(integer) 9
127.0.0.1:6379> pfmerge mykey3 mykey mykey2					# 合并两组 mykey mykey2 =>mykey3,这里是并集(合并两个集合中不重复的元素合并在一起!),
OK
127.0.0.1:6379> pfcount mykey3								# 查看并集的数量
(integer) 15
```

使用场景:我们使用一些网站的计数,就可以用这个Hyperloglog;比如说每个用户登录上来的就放在这个集合里,表示当前用户,并且后续如果放入当前用户,那么在统计的时候都会进行基数统计,也就是不能重复!

如果允许容错,那么一定可以使用`Hyperloglog`!

如果不允许容错,就使用set或者自己的数据类型即可!

优缺点:

1. 使用set方式在录入的时候就会进行比对,而在大数据量的情况下效率就会变低,并且还会占用内存!
2. 使用Hyperloglog内存是固定的!只有12k!

#### 3. Bitmap(位存储)

> 位存储

常见场景:比如统计用户的信息:在B站中就分为活跃的和不活跃的!还有是登录多少人和未登录多少人的!还有一种场景就是打卡,按照之前纯java实现的话就要去设计数据库表,:userid、status(是否打卡)、data这三个字段,而在Redis中可以Bitmap存储,**只有两个状态的都可以用bitmap**!

​	比如记录打卡365天,365天=365bit, 1字节=8bit ,差不多接近于46个字节左右!! 可以发现这样去做是非常省内存的!而且还能让这个效率变得更高!

Bitmap位图->也是一个数据结构!都是用操作**二进制**位来进行记录!就只有0和1两个状态!

Bitmap其实就是通过位运算来表示元素的值



> 测试

![image-20211013144610439](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211013144610439.png)

比如这里以上画图就是一周的打卡考勤,然后我们用Bitmap来记录周一到周日的打卡:

而我们如果要统计打卡的天数就只需要判断多少天是1就好了;

```bash
127.0.0.1:6379> setbit sign 1 1		# 设置星期一已打卡
(integer) 0
127.0.0.1:6379> setbit sign 2 0		# 设置星期二未打卡
(integer) 0
127.0.0.1:6379> setbit sign 3 0
(integer) 0
127.0.0.1:6379> setbit sign 4 0
(integer) 0
127.0.0.1:6379> setbit sign 5 1
(integer) 0
127.0.0.1:6379> setbit sign 6 1
(integer) 0
127.0.0.1:6379> setbit sign 7 0
(integer) 0
# 错误师范
127.0.0.1:6379> setbit sign 1 2		# 第二个参数只能是0或1,因为二进制就是0和1组成的!
(error) ERR bit is not an integer or out of range
127.0.0.1:6379> setbit sign hello 1	# 第一个参数只能为数字偏移量,不能为其他类型
(error) ERR bit offset is not an integer or out of range
```

查看某一天打卡:

```bash
127.0.0.1:6379> getbit sign 2		# 获取星期二是否打卡
(integer) 0
127.0.0.1:6379> getbit sign 7
(integer) 0
127.0.0.1:6379> getbit sign 1		# 获取星期1是否打卡
(integer) 1
```

统计操作,比如统计打卡天数:

```bash
127.0.0.1:6379> bitcount sign		# 统计这周的打卡记录,就可以看到是否有全勤~!统计为1的数量
(integer) 3
```



## 4. 事务



提到事务我们就能第一时间想起MySQL的ACID原则!然后还要一句话就是"要么同时成功,要么同时失败!"->原子性!

**注意**:

1. 在Redis中是没有原子性的!Redis单条命令是保证原子性的!但是Redis`事务不保证原子性的`!因为这个所有的命令都是依次执行,相互之间并没有事务!
2. Redis`事务没有隔离级别的概念`,也就是没有脏读幻读等等!
3. 所有的命令在事务中并`没有被直接被执行`,因为首先在入队的时候没有执行,而是在我们发起执行命令的时候才会执行!执行命令Exec

Redis事务本质:一组命令的集合!一个事务中的所有命令都会被序列化,在事务执行过程中会按照顺序执行!

特性:

1. **一次性**:在队列里面一次性执行
2. **顺序性**:会按照顺序依次执行
3. **排他性**：事务在执行过程中不允许别人干扰的!

执行一系列的命令保证我们的命令在事务中安安全全执行完,所以至少需要这三个特性!

```bash
------队列 set set set 执行 ----- 
```

redis事务分三个阶段:

- 开启事务(multi)
- 命令入队(.....)
- 执行事务(exec)

锁:Redis可以实现乐观锁,Redis有个监视器是watch

理解:redis事务就是将一组命令加入到集合中,然后统一依次执行命令!

> 正常执行事务!

```bash
127.0.0.1:6379> multi						# 开启事务
OK
127.0.0.1:6379> set k1 v1					# 添加命令让其入队,添加到队列中
QUEUED			
127.0.0.1:6379> set k2 v2
QUEUED
127.0.0.1:6379> get k2
QUEUED
127.0.0.1:6379> set k3 v3
QUEUED
127.0.0.1:6379> exec						# 执行命令,可以发现入队的时候并没有执行而是到我们执行命令的时候才是真正的执行,这些命令会一个一个依次性执行完的
1) OK
2) OK
3) "v2"
4) OK
```

注意:事务在执行exec命令后就没了就结束了,需要重新开启事务执行

> 放弃事务!

```bash
127.0.0.1:6379> multi					# 开启事务	
OK
127.0.0.1:6379> set k1 v1				# 往队列中添加执行命令
QUEUED
127.0.0.1:6379> set k2 v2
QUEUED
127.0.0.1:6379> set k4 v4
QUEUED
127.0.0.1:6379> discard					# 取消事务,在队列中的所有命令就不会执行
OK
127.0.0.1:6379> get k4					# 取消事务后命令不会执行,所以这里的get是获取不到的!事务队列中命令都不会执行的 
(nil)
```

> 编译型异常(java中是代码有问题,Redis中是命令有错!),事务中所有的命令都不会被执行!

```bash
127.0.0.1:6379> multi															# 开启事务
OK
127.0.0.1:6379> set k1 v2														# 正常往队列中添加命令
QUEUED
127.0.0.1:6379> set k2 v1
QUEUED
127.0.0.1:6379> set k3 v3
QUEUED
127.0.0.1:6379> getset k3														# 错误的命令:这里会抛出异常但是事务还未停止,这里只是给出语法错误的提示
(error) ERR wrong number of arguments for 'getset' command
127.0.0.1:6379> set k4 v4
QUEUED
127.0.0.1:6379> set k5 v5
QUEUED
127.0.0.1:6379> exec												# 执行命令,发现这里报错了,根据提示发现事务被取消,说明只要队列中命令存在语法异常所有命令都不会被执行!
(error) EXECABORT Transaction discarded because of previous errors.
127.0.0.1:6379> get k5												# 所有的命令都不会被执行,所以这里获取是获取不到的!
(nil)
```



> 运行时异常(java中好比是1/0,而在Redis中:如果事务队列中有命令存在语法性错误,那么在执行命令的时候其他命令是可以正常执行的!),所以就没用原子性这个说法!其中错误命令抛出异常!

```bash
127.0.0.1:6379> multi												# 开启事务
OK
127.0.0.1:6379> set k1 v1											# 添加命令
QUEUED
127.0.0.1:6379> incr k1												# 语法没问题,但是在执行的时候由于i++不能对字符串进行自增1的操作,所以会报错!
QUEUED
127.0.0.1:6379> set k2 v2
QUEUED
127.0.0.1:6379> set k3 v3
QUEUED
127.0.0.1:6379> get k3
QUEUED
127.0.0.1:6379> exec												# 执行命令
1) OK
2) (error) ERR value is not an integer or out of range				# 这里给出提示说参数值不是Integer类型范围,这里虽然报错了,但是不影响其他命令
3) OK
4) OK
5) "v3"
127.0.0.1:6379> get k2												# 这里依然是可以获取到的!
"v2"
```

通过这个编译性异常和运行时异常我们可以得出一个概念,这个概念我们在之前就已经提出过:`Redis单条命令是保持原子性的，但是事务不保证原子性`

举个例子:

​	就好比是我们买车,如果是在买车的时候就发生故障,那么我们就会立即找到销车商就不要了因为还没给钱,这个就是编译时异常:检查命令语法错误,整个事务都会被取消了,命令全部取消;而如果我们在购买车后在行驶的过程中出现故障了,那么这个时候我们已经给了钱,经销商就不给退了,所以自己只能将就跑,这个就是运行时异常:在命令执行的时候如果遇到异常事务正常执行,其他命令依旧正常执行;

### 4.1 乐观锁(重要)

悲观锁:

- 很悲观:认为什么时候都会出问题,做事就会很谨慎,做什么事都会加锁!用完才会解锁,这种方式就会很影响效率!

乐观锁:

- 很乐观:认为什么时候都不会出现问题,所以不会上锁,更新数据去判断一下在此期间是否有人修改过数据,通过version字段去比对!比如说在每次去比对的时候先查询出来数据不上锁就把这个version查出来,然后修改的时候就会把这个version带上,判断是否正确,正确才会提交,这就是乐观锁的操作!
- 在MySQL中获取version并不加锁,更新的时候比较version,每次操作都会带上version!

**而在Redis中使用Watch监控-代替使用乐观锁操作**

> Redis监视测试

正常执行成功!没有出现任何问题-乐观锁

```bash
127.0.0.1:6379> set money 100
OK
127.0.0.1:6379> set out 0
OK
127.0.0.1:6379> watch money					# 监视money这个key
OK
127.0.0.1:6379> multi						# 事务正常结束,数据期间没有发生变动,这个时候就正常执行成功!
OK
127.0.0.1:6379> decrby money 20
QUEUED
127.0.0.1:6379> incrby out 20
QUEUED
127.0.0.1:6379> exec						# 事务正常执行监控就会被取消掉
1) (integer) 80
2) (integer) 20
```

测试多线程修改值,使用watch可以当做redis的乐观锁操作,监视失败!

这里我开两个客户端都连一个服务端:

one客户端线程1:

```bash
127.0.0.1:6379> watch money				# 监视money这个对象
OK
127.0.0.1:6379> multi					# 开启事务
OK
127.0.0.1:6379> decrby money 10
QUEUED
127.0.0.1:6379> incrby out 10
QUEUED
127.0.0.1:6379> exec					# 这里在还未执行完事务之前,线程2直接就把money给修改,然后watch监控的money发生了变动就会立即冻结这个对象,然后整个事务就会提交失败
(nil)
127.0.0.1:6379> get money				# 这里就是被线程2给修改了后的值
"1000"
127.0.0.1:6379> get out					# 事务失效,所以这里的值是之前的!
"20"
```

two客户端线程2:

```bash
127.0.0.1:6379> get money				# 获取当前money
"80"
127.0.0.1:6379> set money 1000			# 直接把money进行修改了!
OK
```

**理解**:在线程1中开启对key的watch监控,然后在事务还未提交前,该key被线程2插进来修改了值,那么在线程1中监控的watch就会让该事务提交失败!

回过头来:乐观锁在拿到money的时候就会去获取version,比如是100,然后更新的时候修改为1000,那么这个version就会被修改,然后在事务中执行命令的时候拿到原来的数据去比较就会发生错误!就会提交失败!

如果要对Redis事务加锁就用watch就好了!



如果修改失败,获取最新的值就好

```bash
127.0.0.1:6379> unwatch				# 如果发现事务执行失败,就先解锁
OK
127.0.0.1:6379> watch money			# 获取最新的值,再次监视 select version
OK
127.0.0.1:6379> multi
OK
127.0.0.1:6379> decrby money 1
QUEUED
127.0.0.1:6379> incrby money 1
QUEUED
127.0.0.1:6379> exec				# 比对监视的值是否发生了变化,如果没有发生变化,那么可以执行成功,如果变量就执行失败!
1) (integer) 999
2) (integer) 1000
```



这里记录一下使用使用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。

​	何谓数据版本？即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现。

在Redis实现乐观锁中使用watch监控每个key都是带有版本号!初始版本号为1;

其他客户端提交数据一次数据库版本号version就进行更新一次，本次事务提交的时候对比版本号（之前提交数据的情况），要是此次版本号低于数据库当前版本号，就会提交失败；

![img](https://gitee.com/miawei/pic-go-img/raw/master/imgs/22a9518f-e355-315f-8d66-d91af4fda723.jpg)

## 5. Jedis

### 5.1 介绍与快速使用

我们之前操作Redis都是通过客户端进行操作,那么这个就是通过Java程序来操作Redis

> 什么是Jedis?

​	是官方推荐的Java连接开发工具!可理解为Java操作Redis的中间件!是一个jar包;如果我们要使用Java操作Redis那么一定要对Jedis很熟悉!

后面用SpringBoot整合的时候直接用xxxTemplates就直接整合使用了,但是我们依然要学习这个,因为我们学习要知其然还要知其所以然!所以我们要从原生JPA看起!

> 测试

1. 导入对应的依赖

   ```xml-dtd
   <!--导入jedis的包-->
   <!-- https://mvnrepository.com/artifact/redis.clients/jedis -->
   <dependency>
       <groupId>redis.clients</groupId>
       <artifactId>jedis</artifactId>
       <version>3.3.0</version>
   </dependency>
   <!--导入阿里巴巴fastjson-->
   <!-- https://mvnrepository.com/artifact/com.alibaba/fastjson -->
   <dependency>
       <groupId>com.alibaba</groupId>
       <artifactId>fastjson</artifactId>
       <version>1.2.75</version>
   </dependency>
   ```

2. 编码测试:

   - 连接数据库
   - 操作命令
   - 断开连接

连接测试:

```java
public class TestPing {
    public static void main(String[] args) {
        // 1.new Jedis对象即可!
        Jedis jedis = new Jedis(new HostAndPort("127.0.0.1",6379));
        // 2.jedis所有的命令就是我们之前学习数据类型哪些基本的命令
        String ping = jedis.ping();
        System.out.println("ping = " + ping);
    }
}
```

输出:

![image-20211013195428497](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211013195428497.png)

### 5.2 常用API

#### 1. 基本命令:

```java
package cn.itesource;

import redis.clients.jedis.HostAndPort;
import redis.clients.jedis.Jedis;

import java.util.Set;

/**
 * @program: Redis-jedis
 * @description:
 * @author: MiaoWei
 * @create: 2021-10-13 19:46
 **/
public class TestPing {
    public static void main(String[] args) {
        Jedis jedis = new Jedis(new HostAndPort("127.0.0.1", 6379));

        System.out.println("清空数据: " + jedis.flushDB());
        System.out.println("判断某个键是否存在: " + jedis.exists("username"));
        System.out.println("新增<'username','MiaoDaWei'>的键值对: " + jedis.set("username", "MiaoDaWei"));
        System.out.println("新增<'password','password'>的键值对: " + jedis.set("password", "password"));
        System.out.println("系统中所有的键如下:  ");
        Set<String> keys = jedis.keys("*");
        System.out.println(keys);
        System.out.println("删除键password: " + jedis.del("password"));
        System.out.println("判断键password是否存在: " + jedis.exists("password"));
        System.out.println("查看键username所存储的值的类型: " + jedis.type("username"));
        System.out.println("随机返回key空间的一个: " + jedis.randomKey());
        System.out.println("重命名key: " + jedis.rename("username", "name"));
        System.out.println("取出改后的name: " + jedis.get("name"));
        System.out.println("按索引选择数据库查询: " + jedis.select(0));
        System.out.println("删除当前选择数据库中的所有key: " + jedis.flushDB());
        System.out.println("返回当前数据库中key的数目: " + jedis.dbSize());
        System.out.println("删除所有数据库中的所有key: " + jedis.flushAll());

    }
}
```

输出:

![image-20211013201637761](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211013201637761.png)

#### 2. String

```java
package cn.itesource;

import redis.clients.jedis.HostAndPort;
import redis.clients.jedis.Jedis;

import java.util.concurrent.TimeUnit;

/**
 * @program: Redis-jedis
 * @description:
 * @author: MiaoWei
 * @create: 2021-10-13 19:46
 **/
public class TestString {
    public static void main(String[] args) {
        Jedis jedis = new Jedis(new HostAndPort("127.0.0.1", 6379));
        jedis.flushDB();

        System.out.println("===================增加数据===================");
        System.out.println(jedis.set("key1", "value1"));
        System.out.println(jedis.set("key2", "value2"));
        System.out.println(jedis.set("key3", "value3"));
        System.out.println("删除键key2: " + jedis.del("key2"));
        System.out.println("获取键key2: " + jedis.get("key2"));
        System.out.println("修改key1: " + jedis.set("key1", "value1Changed"));
        System.out.println("获取key1的值: " + jedis.get("key1"));
        System.out.println("在key3后面加入值: " + jedis.append("key3", "End"));
        System.out.println("key3的值: " + jedis.get("key3"));
        System.out.println("增加多个键值对: " + jedis.mset("key01", "value01", "key02", "value02", "key03", "value03"));
        System.out.println("获取多个键值对: " + jedis.mget("key01", "key02", "key03"));
        System.out.println("获取多个键值对,其中有一项是不存在的: " + jedis.mget("key01", "key02", "key03", "key04"));
        System.out.println("删除多个键值对01与02: " + jedis.del("key01", "key02"));
        System.out.println("获取多个键值对: " + jedis.mget("key01", "key02", "key03"));

        jedis.flushDB();
        System.out.println("===================新增键值对防止覆盖原先值===================");
        System.out.println("不存在key就设置key的value:" + jedis.setnx("key1", "value1")); //可把setnx拆分为 set no :不存在就创建
        System.out.println("不存在key就设置key的value:" + jedis.setnx("key2", "value2"));
        System.out.println("存在key设置value就会失败: " + jedis.setnx("key2", "value2-new"));
        System.out.println("获取key:" + jedis.get("key1"));
        System.out.println("获取key:" + jedis.get("key2"));

        System.out.println("===================新增键值对并设置有效时间===================");
        System.out.println("给key设置有效时间并设置新的value,不管这个key3是否存在: " + jedis.setex("key3", 2, "value3"));	//可把其中的e理解为expire 设置有效时间
        System.out.println("获取这个key3: " + jedis.get("key3"));
        try {
            TimeUnit.SECONDS.sleep(3); //休眠3秒钟
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println("线程休眠3秒钟后获取key3: " + jedis.get("key3"));

        System.out.println("===================获取原值,更新为新值===================");
        System.out.println("获取key2的value并set新的value: " + jedis.getSet("key2", "key2GetSet")); //不管这个key是否存在,都要set新的value,如果存在只是返回之前的value
        System.out.println("获取key2: " + jedis.get("key2"));
        System.out.println("获得key2的值范围内的字串: " + jedis.getrange("key2", 2, 4));
    }
}
```

输出:

![image-20211013205501703](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211013205501703.png)

#### 3. List

```java
package cn.itesource;

import redis.clients.jedis.HostAndPort;
import redis.clients.jedis.Jedis;

/**
 * @program: Redis-jedis
 * @description:
 * @author: MiaoWei
 * @create: 2021-10-13 19:46
 **/
public class TestList {
    public static void main(String[] args) {
        Jedis jedis = new Jedis(new HostAndPort("127.0.0.1", 6379));
        jedis.flushDB();

        System.out.println("================添加一个list================");
        jedis.lpush("collections", "ArrayList", "Vector", "Stack", "HashMap", "WeakHashMap", "LinkedHashMap");
        jedis.lpush("collections", "HashSet");//lpush可理解为以左边为中心,往左添加,然后后续添加将前面的往后移,后续放在前面的位置
        jedis.lpush("collections", "TreeSet");
        jedis.lpush("collections", "TreeMap");
        System.out.println("collections的内容: " + jedis.lrange("collections", 0, -1)); //-1代表倒数第一个元素,-2代表倒数第二个元素
        System.out.println("collections区间0-3的内容: " + jedis.lrange("collections", 0, 3)); //-1代表倒数第一个元素,-2代表倒数第二个元素
        System.out.println("================================");
        //删除列表指定的值,第二个参数为删除的个数(有重复时),后add进去的值先被删除,这类似于出栈
        System.out.println("删除指定元素的个数: " + jedis.lrem("collections", 2, "HashMap"));
        System.out.println("collections的内容: " + jedis.lrange("collections", 0, -1));
        System.out.println("删除下标0-3区间之外的元素: " + jedis.ltrim("collections", 0, 3));
        System.out.println("collections的内容: " + jedis.lrange("collections", 0, -1));
        System.out.println("collections列表出栈(左端): " + jedis.lpop("collections"));
        System.out.println("collections的内容: " + jedis.lrange("collections", 0, -1));
        System.out.println("collections添加元素,从列表右端,与lpush相对应: " + jedis.rpush("collections", "EnumMap"));
        System.out.println("collections的内容: " + jedis.lrange("collections", 0, -1));
        System.out.println("collections列表出栈(右端): " + jedis.rpop("collections"));
        System.out.println("collections的内容: " + jedis.lrange("collections", 0, -1));
        System.out.println("修改指定collections指定下标1的内容: " + jedis.lset("collections", 1, "LinkedArrayList"));
        System.out.println("collections的内容: " + jedis.lrange("collections", 0, -1));
        System.out.println("=========================================");
        System.out.println("collections的长度: " + jedis.llen("collections"));
        System.out.println("获取collections下标为2的元素: " + jedis.lindex("collections", 2));
        System.out.println("=========================================");
        jedis.lpush("sortedList", "3", "6", "2", "0", "7", "4");
        System.out.println("sortedList排序前: " + jedis.lrange("sortedList", 0, -1));
        System.out.println(jedis.sort("sortedList"));
        System.out.println("sortedList排序后: " + jedis.lrange("sortedList", 0, -1));

    }
}
```

输出:

![image-20211013213318116](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211013213318116.png)

#### 4. Set

```java
package cn.itesource;

import redis.clients.jedis.HostAndPort;
import redis.clients.jedis.Jedis;

/**
 * @program: Redis-jedis
 * @description:
 * @author: MiaoWei
 * @create: 2021-10-13 19:46
 **/
public class TestSet {
    public static void main(String[] args) {
        Jedis jedis = new Jedis(new HostAndPort("127.0.0.1", 6379));
        jedis.flushDB();

        System.out.println("=======================向集合中添加元素(不重复)========================");
        System.out.println(jedis.sadd("eleSet", "e1", "e2", "e3", "e4", "e5"));
        System.out.println(jedis.sadd("eleSet", "e6"));
        System.out.println(jedis.sadd("eleSet", "e6"));
        System.out.println("eleSet的所有元素为: " + jedis.smembers("eleSet"));
        System.out.println("删除一个元素e0: " + jedis.srem("eleSet", "e0"));
        System.out.println("eleSet的所有元素为: " + jedis.smembers("eleSet"));
        System.out.println("删除两个元素e7和e6: " + jedis.srem("eleSet", "e7", "e6"));
        System.out.println("eleSet的所有元素为: " + jedis.smembers("eleSet"));
        System.out.println("随机的移除集合中的一个元素: " + jedis.spop("eleSet"));
        System.out.println("随机的移除集合中的一个元素: " + jedis.spop("eleSet"));
        System.out.println("eleSet的所有元素为: " + jedis.smembers("eleSet"));
        System.out.println("eleSet中包含元素的个数为: " + jedis.scard("eleSet"));
        System.out.println("e3是否在eleSet中: " + jedis.sismember("eleSet", "e3"));
        System.out.println("e1是否在eleSet中: " + jedis.sismember("eleSet", "e1"));
        System.out.println("e5是否在eleSet中: " + jedis.sismember("eleSet", "e5"));
        System.out.println("=======================================================");
        System.out.println(jedis.sadd("eleSet1", "e1", "e2", "e4", "e3", "e0", "e8", "e7", "e5"));
        System.out.println(jedis.sadd("eleSet2", "e1", "e2", "e4", "e3", "e0", "e8"));
        System.out.println("将eleSet1中删除e1并存入eleSet3中: " + jedis.smove("eleSet1", "eleSet3", "e1"));
        System.out.println("将eleSet1中删除e2并存入eleSet3中: " + jedis.smove("eleSet1", "eleSet3", "e2"));
        System.out.println("eleSet1中的元素: " + jedis.smembers("eleSet1"));
        System.out.println("eleSet3中的元素: " + jedis.smembers("eleSet3"));
        System.out.println("=================集合运算==============================");
        System.out.println("eleSet1中的元素: " + jedis.smembers("eleSet1"));
        System.out.println("eleSet2中的元素: " + jedis.smembers("eleSet2"));
        System.out.println("eleSet1和eleSet2的交集: " + jedis.sinter("eleSet1", "eleSet2"));
        System.out.println("eleSet1和eleSet2的并集: " + jedis.sunion("eleSet1", "eleSet2"));
        System.out.println("eleSet1和eleSet2的差集: " + jedis.sdiff("eleSet1", "eleSet2")); //eleSet1中有,eleSet2中没有
        jedis.sinterstore("eleSet4", "eleSet1", "eleSet2"); //求交集并将交集保存到dstkey集合
        System.out.println("eleSet4中的元素: " + jedis.smembers("eleSet4"));
    }
}
```

输出:

![image-20211014182406100](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211014182406100.png)

#### 5.Hash

```java
package cn.itesource;

import redis.clients.jedis.Jedis;

import java.util.HashMap;
import java.util.Map;

public class TestHash {
    public static void main(String[] args) {
        Jedis jedis = new Jedis("127.0.0.1", 6379);
        jedis.flushDB();
        Map<String, String> map = new HashMap<String, String>();
        map.put("key1", "value1");
        map.put("key2", "value2");
        map.put("key3", "value3");
        map.put("key4", "value4");
        map.put("key4", "value4"); //不能重复
        //添加名称为hash（key）的hash元素
        jedis.hmset("hash", map); // 批量设置
        //向名称为hash的hash中添加key为key5，value为value5元素
        jedis.hset("hash", "key5", "value5");
        System.out.println("散列hash的所有键值对为：" + jedis.hgetAll("hash"));//return Map<String,String>
        System.out.println("散列hash的所有键为：" + jedis.hkeys("hash"));//return Set<String>
        System.out.println("散列hash的所有值为：" + jedis.hvals("hash"));//return List<String>
        System.out.println("将key6保存的值加上一个整数，如果key6不存在则添加key6：" + jedis.hincrBy("hash", "key6", 6));
        System.out.println("散列hash的所有键值对为：" + jedis.hgetAll("hash"));
        System.out.println("将key6保存的值加上一个整数，如果key6不存在则添加key6：" + jedis.hincrBy("hash", "key6", 3));
        System.out.println("散列hash的所有键值对为：" + jedis.hgetAll("hash"));
        System.out.println("删除一个或者多个键值对：" + jedis.hdel("hash", "key2"));
        System.out.println("散列hash的所有键值对为：" + jedis.hgetAll("hash"));
        System.out.println("散列hash中键值对的个数：" + jedis.hlen("hash"));
        System.out.println("判断hash中是否存在key2：" + jedis.hexists("hash", "key2"));
        System.out.println("判断hash中是否存在key3：" + jedis.hexists("hash", "key3"));
        System.out.println("获取hash中的值：" + jedis.hmget("hash", "key3"));
        System.out.println("获取hash中的值：" + jedis.hmget("hash", "key3", "key4"));
    }
}
```

输出:

![image-20211014183220178](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211014183220178.png)

#### 6. Zset与其他特殊类型

这里我就不贴代码了,直接看吧是不是都有对应的API:

Zset:

![image-20211014183829980](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211014183829980.png)

geo:

![image-20211014183852518](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211014183852518.png)

Hyperloglog:

![image-20211014183934459](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211014183934459.png)

Bitmap:

![image-20211014184015586](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211014184015586.png)

> 通过这个我们可以发现:这里的API跟我们之前敲的命令一模一样!甚至可以说照搬过去了,所以我们学了命令再去看这个就一目了然了!

#### 7.事务

这是正常执行的:

```java
public static void main(String[] args) {
        Jedis jedis = new Jedis(new HostAndPort("127.0.0.1", 6379));

        JSONObject jsonObject = new JSONObject();
        jsonObject.put("hello", "world");
        jsonObject.put("hello1", "world1");
        String jsonString = jsonObject.toJSONString();
        //开启事务
        Transaction transaction = jedis.multi();

        try {
            transaction.set("user1", jsonString);
            transaction.set("user2", jsonString);

            transaction.exec(); //执行事务
        } catch (Exception e) {
            transaction.discard(); //放弃事务
            e.printStackTrace();
        } finally {
            System.out.println(jedis.get("user1"));
            System.out.println(jedis.get("user2"));
            //关闭连接
            jedis.close();
        }
    }
```

输出:

![image-20211014185419371](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211014185419371.png)

这是发生编译时异常:

```java
package cn.itesource.Transaction;

import com.alibaba.fastjson.JSONObject;
import redis.clients.jedis.HostAndPort;
import redis.clients.jedis.Jedis;
import redis.clients.jedis.Transaction;

/**
 * @program: Redis-jedis
 * @description:
 * @author: MiaoWei
 * @create: 2021-10-14 18:43
 **/
public class TestTx {
    public static void main(String[] args) {
        Jedis jedis = new Jedis(new HostAndPort("127.0.0.1", 6379));
        jedis.flushDB(); //防止上一次的数据保存
        JSONObject jsonObject = new JSONObject();
        jsonObject.put("hello", "world");
        jsonObject.put("hello1", "world1");
        String jsonString = jsonObject.toJSONString();
        //开启事务
        Transaction transaction = jedis.multi();
        try {
            transaction.set("user1", jsonString);
            transaction.set("user2", jsonString);
            int i = 1 / 0;  //代码执行失败,抛出异常,事务执行失败!
            transaction.exec(); //执行事务
        } catch (Exception e) {
            transaction.discard(); //放弃事务
            e.printStackTrace();
        } finally {
            System.out.println(jedis.get("user1"));
            System.out.println(jedis.get("user2"));
            //关闭连接
            jedis.close();
        }
    }
}
```

输出:

![image-20211014185813550](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211014185813550.png)

可以发现一旦编译时发生异常,抛出了异常那么,整个事务都会被取消执行失败!

而我们后面实现乐观锁也是一样的,我们只需要加一个`jedis.watch(xxx)`即可!

## 6.Redis.config

这里是介绍Redis在启动的时候是如何进行启动的,而通过它的配置文件,我们来了解一下具体的使用:

我们学东西,如果你只会上面的那些基本的东西,那么你压根不能懂Redis!而这配置文件里的东西才是我们需要去了解的!通过这个配置文件我们就能redis几乎所有的功能!

我们打开Redis所在的目录下的配置文件,比如我这里的是`redis.windows.conf`打开进行解析:

> 单位:

```properties
# Redis configuration file example

# Note on units: when memory size is needed, it is possible to specify
# it in the usual form of 1k 5GB 4M and so forth:
# 这里是默认的一些单位设置!
# 1k => 1000 bytes
# 1kb => 1024 bytes
# 1m => 1000000 bytes
# 1mb => 1024*1024 bytes
# 1g => 1000000000 bytes
# 1gb => 1024*1024*1024 bytes
#
# units are case insensitive so 1GB 1Gb 1gB are all the same. 这里翻译:这里对大小写不敏感,也就是说你可以写1GB,也可以写1gb....
这里是Redis的默认的一些配置,
################################## INCLUDES ###################################
```

1. 配置文件unit单位,对大小写不敏感

> 包含:

```properties
################################## INCLUDES ###################################

# Include one or more other config files here.  This is useful if you
# have a standard template that goes to all Redis servers but also need
# to customize a few per-server settings.  Include files can include
# other files, so use this wisely.
#
# Notice option "include" won't be rewritten by command "CONFIG REWRITE"
# from admin or Redis Sentinel. Since Redis always uses the last processed
# line as value of a configuration directive, you'd better put includes
# at the beginning of this file to avoid overwriting config change at runtime.
#
# If instead you are interested in using includes to override configuration
# options, it is better to use include as the last line.
#
# include .\path\to\local.conf
# include c:\path\to\other.conf
```

就是好比我们学习Spring的时候import,或者JSP中的include,这里意思就是可以把多个配置文件都配置过来!

> 网络:

```bash
################################## NETWORK #####################################
# ... 这里我省略其他,直接看核心
bind 127.0.0.1		//绑定的ip
protected-mode yes  //表示是否是受保护的模式,一般都是开启的,保证安全性的!
port 6379			//绑定的端口 
```

> 通用配置:

```bash
################################# GENERAL #####################################
# ... 这里我省略其他,直接看核心
daemonize yes    # 表示是否以守护进程开启,默认为no,我们需要自己开启为yes!就是退出还在后台运行!
supervised no	 # 这个就是管理守护线程的,不用去动
pidfile /var/run/redis.pid # 如果以后台的方式运行,我们就需要指定一个pid文件!

# 日志
# Specify the server verbosity level.
# This can be one of:
# debug (a lot of information, useful for development/testing)  //这里位debug,一般用于测试和开发阶段!
# verbose (many rarely useful info, but not a mess like the debug level) //这里就是记录较多的日志信息.一般不用去看
# notice (moderately verbose, what you want in production probably)	//通知,部分的重要的日志信息.仅使用与生产环境使用!
# warning (only very important / critical messages are logged)	//警告信息.关键的信息!
loglevel notice
logfile ""  # 日志的文件位置名,如果为空,那么就标准的输出了!
databases 16  # 数据库数量,默认的是16个数据库
always-show-Logo yes  # 是否显示这个logo,就是我们开启Redis服务的时候显示的那个logo
```

> 快照(后面持久化会用到,就是在规定的时间内,执行了多少次操作,则会持久化到文件 .rdb aof):

```bash
################################ SNAPSHOTTING  ################################
# save: 在当下执行了多少时间怎样去保存这个规则->这是个持久化规则
# Redis是一个内存式数据库,如果不持久化的话就会丢失数据,因为内存是断电即失!
save 900 1 		# 如果900秒内也就是15分钟,如果至少一个key进行了修改,那么这个时候我们就进行持久化操作!
save 300 10		# 假设300秒内,至少10个key进行修改,那么这个时候我们就进行持久化操作!
save 60 10000	# 假设60秒内,至少10000个key被进行修改,那么就会执行持久化操作,这种就是高并发的情况下!

stop-writes-on-bgsave-error yes  	# 如果持久化的时候出现错误,是否还继续工作,默认是开启
rdbcompression yes					# 是否压缩我们的rdb文件,默认是开启,压缩的话就会消耗一些CPU资源,而rdb文件就是持久化的文件
rdbchecksum yes						# 在保存rdb文件的时候,就会去校验我们的rdb文件,如果出错了自动去做一些操作修复!
dir ./								# rdb文件保存的目录!
```

> 安全

```bash
################################## SECURITY ###################################
requirepass 123456		# 设置redis密码,默认是为空,一般我们推荐使用命令行设置命令
```

密码的话我们也可以进行设置,当然默认为空,那我们在cmd中查看用户密码那如何查看呢?

```bash
127.0.0.1:6379> config get requirepass					# 获取redis密码
1) "requirepass"
2) ""
127.0.0.1:6379> config set requirepass "123456"			# 设置redis密码
OK
127.0.0.1:6379> config get requirepass					# 设置后我们就能看到必须要登录,否则就没用权限,不然我们所有的命令都无法使用!
(error) NOAUTH Authentication required.
127.0.0.1:6379>  config get requirepass
(error) NOAUTH Authentication required.
127.0.0.1:6379> ping
(error) NOAUTH Authentication required.					# 验证密码
127.0.0.1:6379> auth 123456
OK
127.0.0.1:6379>  config get requirepass					# 获取密码就可以看到了!
1) "requirepass"
2) "123456"
```

> 客户端限制

```bash
################################### LIMITS ####################################

# Set the max number of connected clients at the same time. By default
# this limit is set to 10000 clients, however if the Redis server is not
# able to configure the process file limit to allow for the specified limit
# the max number of allowed clients is set to the current file limit
# minus 32 (as Redis reserves a few file descriptors for internal uses).
#
# Once the limit is reached Redis will close all the new connections sending
# an error 'max number of clients reached'.
#
# maxclients 10000   # 设置能连接redis最大客户端的数量
...
# maxmemory <bytes>		# redis最大的内存容量
...
# maxmemory-policy noeviction  # 内存达到上限之后的处理策略
	1、volati1e-lru：只对设置了过期时间的key进行LRU（默认值）,把一些过期key进行移除!
	2、allkeys-lru：删除lru算法的key;	删除移除算法的key
	3、volatile-random：随机删除即将过期key; 
	4、allkeys-random：随机删除
	5、vo1atile-ttl：删除即将过期的
	6、noeviction：永不过期，返回错误
```

> APPEND ONLY MODE AOF的配置

```bash
############################## APPEND ONLY MODE ###############################
appendonly no	# 默认是不开启AOF模式的,默认是使用RDB方式持久化的,因为大部分所有的情况下RDB已经够用了!
appendfilename "appendonly.aof"		# 持久化的文件名字

# appendfsync always			# 每次修改都会同步,这会消耗性能,因为每次都要去写
appendfsync everysec			# 每秒执行一次sync,可能会丢失这1秒的数据!
# appendfsync no				# 不同步,不执行sync(异步是ASYNC),这个时候操作系统自己同步数据,速度是最快的!

no-appendfsync-on-rewrite no 	# 重写的规则:重写的时候是否运行append同步写入,默认是no就可以了,可以保证数据的安全性!
auto-aof-rewrite-percentage 100	# 重写的基准是100
auto-aof-rewrite-min-size 64mb	# 重写的大小为65mb,
```

## 7. Redis持久化

Redis是内存数据库,如果不将内存中的数据库状态保存到磁盘,那么一旦服务器进程退出,服务器中的数据库状态也会消失,所以Redis提供了持久化功能!

### 7.1 RDB(Redis DataBase)

> 什么是RDB

![image-20211016084629879](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016084629879.png)

**简述**:在指定的时间间隔内将内存中的数据集快照写入磁盘,也就是行话说的Snapshot快照,它恢复时是将快照文件直接读到内存里!

**流程**:Redis会单独创建(fork)一个子进程来进行持久化,而这个子进程会将内存数据写入到一个临时文件中,也就是临时的RDB文件,然后等待持久化过程都结束了,再用这个临时文件代替上次持久化的文件。那么就会生成一个正式的RDB文件。

**优点**:整个过程中，主进程是不用进行任何IO操作的，这就确保了极高的性能。如果需要进行大规模数据的恢复，且对于数据恢复的完整不是非常敏感，那RDB方式要比AOF方式更加的高效，

**缺点**:RDB的缺点是最后一次持久化的数据可能丢失,因为宕机就无法去保持持久化操作;

**我们默认的就是RDB,一般情况下不需要修改这个配置!**

rdb保存的文件是`dump.rdb`,都是在我们的配置文件中快照中进行配置的:

![image-20211016090528766](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016090528766.png)

> 测试:

1. 我们去配置文件中修改快照中对于RDB文件的持久化规则

   ```bash
   # save 900 1
   # save 300 10
   # save 60 10000
   save 60 5  # 这是自定义的设置,只要我在60秒内修改了5次key,就会触发RDB操作!
   ```

2. 我们快速写五个命令,这个时候我们去Redis文件夹中可以看到:

   ![image-20211016091738923](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016091738923.png)

> 触发机制

1. save命令的规则满足的情况下会自动触发RDB规则
2. 执行了flushALL命令,也会触发我们的RDB规则!
3. 退出redis,也会产生RDB文件!

备份完后就会自动生成一个dump.rdb

> 如何恢复RDB文件!

1. 只需要将RDB文件放在我们的Redis的启动目录下,redis启动的时候会自动去检查dump.rdb文件,恢复其中的数据!

2. 查看需要存放的位置

   ```bash
   127.0.0.1:6379> config get dir
   1) "dir"
   2) "F:\\Redis\\Redis"		# 如果在这个目录下存在dump.db文件,那么启动的时候就会自动恢复其中的数据
   ```

> 优缺点:

优点:

1. 适合大规模的数据恢复!dump.rdb 
2. 如果对数据的完整性要求不高!因为在配置文件中设置RDB规则比如60秒修改5个key就触发RDB操作,那么在第4个key修改的时候就宕机了,那么这最后一次修改的数据就会没持久化到RDB文件中去!

缺点:

1. 需要一定时间间隔去操作!
2. fork进程的时候会占用一定的内存空间!所以我们会要考虑一下内存空间的问题!

### 7.2 AOF(Append Only File)

将我们所有的命令都记录下来,就好比是一个history文件,那恢复的时候就会把这个文件全部执行一遍!
![image-20211016095641596](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016095641596.png)

以日志的形式来记录每个**写操作**,将Redis执行过的所有命令都记录下来(读操作不记录),只许追加文件但不可以改写文件,redis启动之初就会读取该文件重新构建数据,换言之,redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作!

`AOF保存的是appendonly.aof文件`

> 配置文件

在配置文件中关于AOF的配置在:

![image-20211016100236460](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016100236460.png)

修改配置文件:

```bash
appendonly yes		# 默认是不开启的,现在手动改为开启状态
appendfilename "appendonly.aof"		# 这是生成的文件名,这里最好不要改!

# 持久化的策略		
# appendfsync always		# 每一次都要去修改
appendfsync everysec		# 每一秒钟都要去执行一次!
# appendfsync no			# 不修改
```

实际上我们只需要开启AOF即可,其他的一律默认就行!

> 测试

1. 现在我们连上服务器,就可以看见已经生成了aof文件:

   ![image-20211016103906076](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016103906076.png)

2. 然后现在我们打开客户端,然后执行命令`set k1 v2`,然后我们去看aof文件,因为持久化的策略就是每秒中都要去执行一次

   ![image-20211016104130699](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016104130699.png)

   从图中我们可以看出几个关键词,比如最后一个的v1和k1,这不就是我们刚刚的的set命令吗!也就是写操作!

3. 那如果我们破坏性的去这个配置文件中进行修改,那试想一下启动会怎么样呢?我把最后一行改为`aaaaaaav1`,此时我们开启服务器:

   ![image-20211016105005370](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016105005370.png)

   **说明**:不能进行连接,刷新失败!所以说AOF有问题,是不能进行启动的!

   **解决**:我们需要修复这个AOF文件,Redis为我们提供了这样的一个工具进行修复!`redis-check-aof`

   **注意:**RDB是没有修复的,要记住,RDB根本就不是写操作的,而是将内存中的数据写在文件中,我们打开时根本看不懂的!

4. 修复:

   ![image-20211016105811804](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016105811804.png)

   这样我们的AOF文件就可以正常使用了,

5. 我们完整性的测试一下这个修复的!

   ```bash
   命令行:
   set k1 v2
   set k2 v1
   ```

   配置文件:

   ![image-20211016110156952](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016110156952.png)

   把最后一行修改为`aaaaaaaaaaaaaaaaaaaav1`,然后使用命令行进行修复这个文件:

   ```bash
   F:\Redis\Redis>redis-check-aof --fix appendonly.aof
   0x              64: Expected \r\n, got: 6161
   AOF analyzed: size=124, ok_up_to=75, diff=49
   This will shrink the AOF from 124 bytes, with 49 bytes, to 75 bytes
   Continue? [y/N]: y
   Successfully truncated AOF
   ```

   然后我们再去看配置文件:

   ![image-20211016110322777](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016110322777.png)

   可以发现他把最后那一行给清除掉了,准确的来是把有关那行错误的命令给清除了,那此刻我们回到服务器上查看还有哪些key:

   ```bash
   127.0.0.1:6379> keys *
   1) "k1"
   ```

   完美!也就是说只会丢弃错误的命令东西!

> 优点和缺点

优点:

1. 每一次修改都同步!这样的好处就是文件的完整性更好!
2. 每秒同步一次,可能会丢失一秒的数据!
3. 从不同步,效率就是最高的!

缺点:

1. 相对于数据文件来说,AOF远远大于RDB!修复的速度也比RDB慢! 
2. AOF运行效率要比RDB慢,所以我们redis默认的配置就是RDB持久化!

> 重写规则

我们之前在配置文件中看到对重写的规则!

![image-20211016111313008](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016111313008.png)

如果AOF文件大于64M,太大了,那么就会fork一个新的进程来将我们的文件进行重写!

**记住:**AOF默认的就是文件无限制追加,文件就会越来越大!所以才会出现相应的规则与策略!

### 7.3 扩展

1. RDB持久化方式能够在指定的时间间隔内对你的数据进行快照存储
2. AOF持久化方式会记录你每次服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF命令以Redis协议追加保存每次写的操作到文件末尾,Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大
3. `只做缓存:如果你只希望你的数据在服务器运行的时候存在,你也可以不使用任何持久化`
4. 同时开启两种持久化方式
   - 在这种情况下,当redis重启的时候会优先载入AOF文件来恢复原始的数据,因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集更加完整,而且RDB还有一些自己的规则!
   - RDB的数据不实时,同时使用两者时服务重启也只会找AOF文件,那要不要只使用AOF呢?建议不要,因为RDB更适合用于备份数据库(AOF在不断变化不好备份),快速重启,而且不会有AOF可能存在潜在的BUG,留着作为一个万一的手段!
5. 性能建议
   - 因为RDB文件只用作后备用途,建议只在Slave(从机)上持久化RDB文件,而且只要15分钟备份一次就够了,只保留save 900 1 这条规则
   - 如果Enable AOF,好处是在最恶劣情况下也只会丢失不超过两秒数据,启动脚本较简单只load自己的AOF文件就可以了,代价一是带来了持续的IO,二是AOF rewrite过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的,只要硬盘许可,应该尽量减少AOF rewrite的频率,AOF重写的基础大小默认值46M太小了,可以设到5G以上,默认超过原大小100%大小重写可以改到适当的数值。
   - 如果不Enable AOF,仅靠Mster-Slave Repllcation(主从复制)实现高可用性可以,能省掉一大笔IO,也减少了rewrite时带来的系统波动,代价是如果Master/Slave(主机和从机)同时宕机(比如断电),会丢失十几分钟的数据,启动脚本也要比较两个Mster/Slave中的RDB文件,载入较新的那个(占用内存更大),微博就是这种架构.

## 8.Redis发布订阅

### 8.1 概述

Redis发布订阅(pub/sub)是一种**消息通信模式**:发送者(pub)发送消息,订阅者(sub)接收消息,类似于微信,微博,关注系统!

Redis客户端可以订阅任意数量的频道。

理解:一个人可以关注很多的博主

订阅/发布消息图:

核心:第一个:消息发送者,第二个:频道 第三个:消息订阅者!

![image-20211016134920273](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016134920273.png)

下图展示了频道channel1,以及订阅这个频道的三个客户端--client2、client5和client1之间的关系:

![image-20211016135141675](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016135141675.png)

当有新消息通过publish命令发送给频道channel1时,这个消息就会被发送给订阅它的三个客户端:

![image-20211016135257186](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016135257186.png)

### 8.2 命令

> 命令

这些命令被广泛用于构建即时通信应用,比如网络聊天室(charroom)和实时广播、实时提醒等!

![image-20211016135931447](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016135931447.png)

### 8.3 测试

1. 发送者

   ```bash
   127.0.0.1:6379> publish MiaoDaWei "Hello"			# 发布订阅,给指定频道发送消息
   (integer) 1
   127.0.0.1:6379> publish MiaoDaWei "Redis"
   (integer) 1
   ```

2. 订阅者

   ```bash
   127.0.0.1:6379> subscribe MiaoDaWei					# 订阅指定频道.开始监听,只有发送者发送发消息,订阅者这边就能接收
   Reading messages... (press Ctrl-C to quit)
   1) "subscribe"
   2) "MiaoDaWei"
   3) (integer) 1
   # 等待读取推送的信息
   1) "message"	# 信息
   2) "MiaoDaWei"	# 哪个频道的消息
   3) "Hello"		# 接收的消息的具体内容
   1) "message"
   2) "MiaoDaWei"
   3) "Redis"
   ```

   

注意:要先建立订阅者,然后再建立发送者,不然先建立发送者发消息给频道,而频道找不到订阅者,就不会发出去了!

### 8.4 原理

​	Redis是使用C实现的。通过分析Redis源码里pubsub.c文件,了解发布和订阅机制的底层实现,来增加对Redis的理解;

Redis通过publish,subscribe和psubscribe等命令实现发布和订阅功能!

通过subscribe命令订阅某频道后,redis-server里维护了一个`字典`,字典的键就是一个个chanel(`频道`),而字典的值则是一个`链表`,链表中保存了所有订阅这个channel的客户端。Subscribe命令的关键,就是将客户端添加到给定channel的订阅链表中。

通过publish命令往订阅者发送消息,redis-server会使用给定的频道作为键,在它所维护的channel字典中查找记录了订阅这个频道的所有客户端的链表,遍历整个链表,将消息发布给所有订阅者。



Pub/Sub从字面上理解就是发布(Publish)与订阅(Subscribe),在Redis中,你可以设定对某一个key值进行消息发布即消息订阅,当一个key值上进行了消息发布之后,所有订阅它的客户端都会收到相应的消息。这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能！

> 理解：订阅者在开始订阅某个频道的时候，在redis-server中保存了一个字典,这个字典中key就是频道,value就是一个链表,这个链表就是保存了众多订阅者客户端,然后在消息发布者对某一频道发布消息了的时候,就会首先去redis-server中的字典里查找这个频道的key,如果找到那么就会遍历其中的链表,然后个其中的消息订阅者,发送消息!

其实最常见的场景就比如说微信的订阅号,你订阅了之后是不是就会按时给你推送呢?

画了个图,来方便理解一下:

![image-20211016145149681](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016145149681.png)

画得不是很好,理解理解!

**使用场景:**

1. 实时消息系统!
2. 实时聊天!(频道当做聊天室,将信息回显给所有人即可!)
3. 订阅、关注系统都是可以的!

复杂一点的消息的场景就是使用消息中间件来做,比如MQ,毕竟专业!

## 9.Redis主从复制

主从复制拆分为:1个主人带3个仆人->小的集群,这个主人吩咐的事情那这些仆人就去遵守它的命令!

主从复制,读写分离!80%的情况下都是在进行读操作!减缓服务器的压力!在架构中经常使用!经常都是一主二从!

### 9.1 概念

主从复制,是指将一台Redis服务器的数据,复制到其他的Redis服务器,前者称为主节点(master/leader),后者称为从节点(slave/follower);`数据的复制是单向的,只能由主节点到从节点`。Master（主机）以写为主,Slave（从机）以读为主。

`默认情况下，每台Redis服务器都是主节点`;且一个主节点可以有多个从节点(或没有从节点),但一个从节点只能由一个主节点。

主从复制的**作用**主要包括：

1. 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式
2. 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。
3. 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务，（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点）,分担服务器负载;尤其是在写少读多的场景下,通过多个从节点分担读负载,可以大大提供Redis服务器的并发量.
4. 高可用(集群)基石,除了上述作用以外,主从复制还是哨兵和集群能够实施的基础,因此可以说主从复制是Redis高可用的基础!

可以从这张图可以理解一下(Master:主机,Slave:从机):

![image-20211016151742089](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016151742089.png)

一般来说,要将Redis运用于工程项目中,只使用一台Redis是万万不能的(宕机),原因如下:

1. 从结构上,单个Redis服务器会发生单点故障,并且一旦服务器需要处理所有的请求负载,压力比较大!
2. 从容量上来看,单个Redis服务器内存容量有限,就算一台Redis服务器内存容量为256G,也不能将所有内存用作Redis存储内存,一般来说,`单台Redis最大使用内存不应该超过20G`

> 电商网站上的商品,一般都是一次上传,无数次浏览的,说专业点也就是"多读少写"!

记住一句话:只要在公司中,主从复制就是必须要使用的!因为在真实的开发环境中,不可能会单机使用Redis!

### 9.2 环境配置

只配置从库,不用配置主库,因为默认的每台Redis服务都是主库!

```bash
127.0.0.1:6379> info replication		# 查看当前库的信息
# Replication
role:master								# 角色,默认是master
connected_slaves:0						# 从机数量,这里是没有从机
master_repl_offset:0
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
```

接下来我们去配置文件中修改:

1. 修改端口,如:port 6380
2. log文件名字,如:logfile "6380.log"
3. dump.rdb名字,如:dbfilename dump6380.rdb

修改完毕之后,启动我们三个集群,

从cmd命令中进行启动,比如`redis-server.exe redis.windows6381.conf`

然后此时我们在进程中就可以看见三个服务器:

![image-20211016155827271](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016155827271.png)

然后我们分别启动对应的客户端:

比如启动`redis-cli -p 6379`,这样就会针对客户端连接指定端口!

> 然后开始配置我们的一主二从

这里要注意一个点:`默认情况下,每台Redis服务器都是主节点;`我们一般情况下只用配置从机就好了!
那如何配置呢?我们首先就来想一下一主二从就仿佛就是在认老大一样,我们只需要去从机上配置一下就可以了!

```bash
# 从机6380
127.0.0.1:6380> slaveof 127.0.0.1 6379				# slaveof host 6379 去找谁当自己的老大
OK
127.0.0.1:6380> info replication
# Replication			
role:slave											# 当前的角色是从机
master_host:127.0.0.1								# 主机的host
master_port:6379									# 主机的端口
master_link_status:down
master_last_io_seconds_ago:-1
master_sync_in_progress:0
slave_repl_offset:1
master_link_down_since_seconds:jd
slave_priority:100
slave_read_only:1
connected_slaves:0
master_repl_offset:0
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0

# 从机6381
127.0.0.1:6381> slaveof 127.0.0.1 6379
OK
127.0.0.1:6381> info replication
# Replication
role:slave
master_host:127.0.0.1
master_port:6379
master_link_status:down
master_last_io_seconds_ago:-1
master_sync_in_progress:0
slave_repl_offset:1
master_link_down_since_seconds:jd
slave_priority:100
slave_read_only:1
connected_slaves:0
master_repl_offset:0
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
# 主机上看:
127.0.0.1:6379> info replication								# 主机查看信息
# Replication
role:master
connected_slaves:2
slave0:ip=127.0.0.1,port=6381,state=online,offset=71,lag=0		# 可以看到从机都有哪些
slave1:ip=127.0.0.1,port=6380,state=online,offset=71,lag=0
master_repl_offset:71
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:2
repl_backlog_histlen:70
```

踩坑记录:

如果遇到从机连接了主机,但是在主机上却查询不到任何从机的信息,那么这个时候就是你主机设置了密码,导致在连接的时候报了异常错误!解决办法:注释掉主机的密码!

这是解决办法:https://blog.csdn.net/qq_43842093/article/details/117197425



就会发现就有两个从机了!当然查看命令依然是`info replication`;

> 以上命令都是暂时的,如果要永久使用那么我们需要去配置中去修改,这样在启动的时候就直接配置:

![image-20211016162818866](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016162818866.png)

### 9.3 细节

主机可以写,从机不能写,只能读!主机中所有的信息和数据,都会自动被从机保存!

```bash
# 主机
127.0.0.1:6379> set k1 v1		# 主机进行写,然后会自动复制到从机上
OK
# 从机 6380
127.0.0.1:6380> get k1			# 从机就可以获取进行读的操作
"v1"
# 从机 6381
127.0.0.1:6381> get k1
"v1"
```

思考?从机我要是偏要想写呢?

```bash
# 从机 6381
127.0.0.1:6381> set k2 v2
(error) READONLY You can't write against a read only slave.			# 抛出异常,说你只能进行读的操作,因为你是一个从机!
```

如果我在主机上清空当前数据库,那从机是否可以获取呢?

```bash
# 主机 6379
127.0.0.1:6379> flushdb
OK
#从机 6380
127.0.0.1:6380> keys *
(empty list or set)
# 从机 6381
127.0.0.1:6381> keys *
(empty list or set)
# 如果从机清空数据库呢?
127.0.0.1:6380> flushdb
(error) READONLY You can't write against a read only slave.			# 抛出一样的异常,说明了从机只能进行读的操作,不能做任何对数据库进行修改的操作!
```

**测试结论:**主机断开连接,从机依旧连接到主机的,但是没有写操作,这个时候,主机如果回来了,从机依旧可以直接获取到主机写的信息!

```bash
# 主机开始断开
127.0.0.1:6379>shutdown
not connected>exit
# 然后启动,获取一下在断开之前设置的参数值
127.0.0.1:6379> keys *						# 可以发现在主机宕机了之后,从机依然持有数据信息,然后在主机启动了之后依旧能拿到这些值!
1) "k1"
```

**思考**:如果突然从机宕机了,然后主机写入了一条数据,然后过一段时间从机上线,此时能否拿到数据

```bash
# 从机6381开始宕机
127.0.0.1:6381> shutdown
(0.50s)
not connected> exit
# 主机开始写入数据
127.0.0.1:6379> set name miaowei
OK
# 从机6381开始上线
F:\Redis\Redis>redis-cli -p 6381			# 连接到客户端
127.0.0.1:6381> get name					# 获取key没有
(nil)
127.0.0.1:6381> info replication			# 查看配置信息,可以发现这里默认为主机
# Replication
role:master
connected_slaves:0
master_repl_offset:0
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
127.0.0.1:6381> slaveof 127.0.0.1 6379
OK
127.0.0.1:6381> get name					# 配置了从机找到了老大,此时我们发现,依然能够获取key
"miaowei"
```

如果是使用了命令行,来配置的主从,这个时候重启了,就会变回主机!只要变为从机,立马就会从主机中获取值!



### 9.4 复制原理

Slave(从机)启动成功连接到`master`(主机)后会发送一个`sync`(同步)命令

Master接到命令,启动后台的存盘进程,同时`收集所有接收到的用于修改数据集命令`,在后台进程执行完毕之后,`master将传送整个数据文件到slave,并完成一次完全同步!`

> 理解:从机在启动连接主机的时候,主动发起一个同步命令,然后主机就会启动一个进程用于存储修改数据集的命令,然后收集完毕就将这个文件发送给从机,完成完全同步!

**全量复制**:而salave服务在接收数据库文件数据后,将其存盘并加载到内存中!

**增量复制**:Master继续将新的所有收集到的修改命令依次传给slave,完成同步

> 理解:
>
> 全量就是在启动连接的时候就会完成一次全部同步!就是所有的修改命令同步到从机上执行;
>
> 增量就是主机没进行修改添加操作的时候,主机就会把修改命令传给从机,完成一次同步!

注意:只要是重新连接master,一次完全同步(全量复制)将被自动执行!我们的数据一定可以在从机中看到!

### 9.5 宕机配置

这是原本的主从模型:

![image-20211016182456740](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016182456740.png)

也可以用这个模型:

> 层层链路模型(名字自己想的)

![image-20211016182755119](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016182755119.png)

上一个主机链接下一个从机!

这个时候也可以完成我们的主从复制!也就是说跟上面并没有区别,只是说更换了从机与从机之间的链路过程!



**思考**:如果说主机突然出现宕机了,那么后面的从机是否都是只能进行读的操作,那么此时写的操作就无法进行下去:

> 如果没有老大了,这个时候就不能选择一个老大出来呢?手动! 四个字概括"谋朝篡位"

解决:如果主机断开了连接,我们可以使用`slaveof no one`来让自己变成主机!其他的节点就可以手动连接到最新的这个主节点!(哨兵模式之前就是`手动`!)

```bash
127.0.0.1:6381> slaveof no one		# 手动让自己称为主节点,谋朝篡位!
OK
127.0.0.1:6381> info replication
# Replication
role:master
connected_slaves:0
master_repl_offset:546
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
```

**注**:如果此时原来的主机宕机修复了,此时上线依然是主机不过你也可以理解为它是一个光杆司令了,它只能乖乖的跟着现在的主机的屁股后面!这个跟上面的链路模型搭配的起来要好一点,因为是层层链路的关系,所以但凡前面的节点是宕机了,那么额只需要手动修改,就可以重新当主机了,而原来的主机就可以重新链接到最后一层!

## 10.哨兵模式

引入:之前在使用主从复制中对于如果主机宕机了,我们需要进行手动的修改为主机,这是一件非常麻烦的一件事!那么哨兵模式就诞生了,目的就是`自动选举老大的模式`

### 10.1 概述和原理

​	主从切换技术的方法是:当主服务器宕机后,需要手动把一台从服务器切换为主服务器,这就需要人工干预,费事费力,还会造成一段时间内服务不可用,这不是一种推荐的方式,更多时候,我们优先会考虑哨兵模式,Redis从2.8正式提供了Sentinel(哨兵)架构来解决这个问题!

谋朝篡位的自动版,能够后台监控主机是否故障,如果故障了根据投票数`自动将从库转换为主库`。

哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令,哨兵是一个独立的进程,作为进程,它会独立运行!

**原理:哨兵通过发送命令,等待Redis服务器响应,从而监控运行的多个Redis实例。**

这是一个基本模型:

![image-20211016190520522](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016190520522.png)



这里哨兵有两个作用:

1. 通过发送命令,让Redis服务器返回监控其运行状态,包括主服务器和从服务器
2. 当哨兵检测到master宕机,会自动将slave切换成master,然后通过`发布订阅模式`通知其他的从服务器,修改配置文件,让它们切换主机。



**思考**:如果一个哨兵死了怎么办呢?

然而一个哨兵进程对Redis服务器进行监控,可能出现问题,为此,我们可以使用多个哨兵进行监控,各个哨兵之间还会进行监控,这样就形成了多哨兵模式!

![image-20211016191246006](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016191246006.png)

假设主服务器宕机,哨兵1会先检测到这个结果,系统并不会马上进行failover过程,仅仅是哨兵1主观的认为主服务器不可用,这个现象称为`主观下线`。当后面的哨兵也检测到主服务器不可用的时候，并且数量达到一定值的时候，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行failover[故障转移]操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为`客观下线`。

**理解**：比如图中master主机宕机了,此时一个哨兵发现了这个服务器可能出现了问题,但是并不会立马进行一个重新选举的一个过程,因为这仅仅就是一个哨兵发现的,这就是代表一个人的想法,这是不行的,所以这就是`主观下线`。如果其他哨兵检测这个服务器不可用的时候,并且哨兵的检查的数量达到一个值,比如有3个哨兵,要求至少2个哨兵检查到了问题,那么才会真的认为这个服务器是宕机了,那么就会真的断定主机彻彻底底不可用了,那么这个时候哨兵之间就会进行一个投票,发起者可能是任意的一个哨兵,结果就是会进行故障转移操作,转移之后,就会通过发布订阅模式,好比是通知其他从服务器说你们的老大挂掉了,然后各个哨兵之间就会给剩余服务器进行投票,这个操作就好比美国选总统一样,然后比如说从机1被选举出来了,然后把之前的主机进行干掉!这个操作就是`客观下线`;

**注**:关于投票的话,就是一个投票算法!



### 10.2 测试

此时我们的状态是一主二从

注意:哨兵配置文件不能写错,哪怕文件名或者文件里的参数名字,因为在redis启动的时候就会去检测这个文件! 

1. 配置哨兵配置文件`sentinel.conf`

```bash
# sentinel monitor 被监控的名称  host 端口 
sentinel monitor myredis 127.0.0.1 6379 1  # 后面的这个数字1,代表主机挂了,slave投票看让谁接替成为主机,票数最多的,就会成为主机
....
# 配置很多,只是列出上面最核心的东西
```

2. 启动哨兵!

```bash
F:\Redis\Redis>redis-server.exe sentinel.conf --sentinel
                _._
           _.-``__ ''-._
      _.-``    `.  `_.  ''-._           Redis 3.2.100 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._
 (    '      ,       .-`  | `,    )     Running in sentinel mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 26379
 |    `-._   `._    /     _.-'    |     PID: 18468
  `-._    `-._  `-./  _.-'    _.-'
 |`-._`-._    `-.__.-'    _.-'_.-'|
 |    `-._`-._        _.-'_.-'    |           http://redis.io
  `-._    `-._`-.__.-'_.-'    _.-'
 |`-._`-._    `-.__.-'    _.-'_.-'|
 |    `-._`-._        _.-'_.-'    |
  `-._    `-._`-.__.-'_.-'    _.-'
      `-._    `-.__.-'    _.-'
          `-._        _.-'
              `-.__.-'

[18468] 16 Oct 19:46:42.365 # Sentinel ID is 74535147bc67d3cec2bca767558b8e6602940710
[18468] 16 Oct 19:46:42.369 # +monitor master myredis 127.0.0.1 6379 quorum 1							# 这里看出它正在监视这个主服务器,并且有一票,说明它是老大
[18468] 16 Oct 19:46:42.370 * +slave slave 127.0.0.1:6380 127.0.0.1 6380 @ myredis 127.0.0.1 6379		# 可以看出它由两个从服务器!
[18468] 16 Oct 19:46:42.370 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ myredis 127.0.0.1 6379
[18468] 16 Oct 19:47:12.400 # +sdown slave 127.0.0.1:6381 127.0.0.1 6381 @ myredis 127.0.0.1 6379
```

3. 测试

   如果Master主节点断开了,这个时候就会从Slave子节点中随机选择一个服务器!(这里面有一个投票算法,暂时不去探究)

   1. 我把主节点开始断开,此时需要等待几秒再去看哨兵日志:

      ```bash
      # 这里就是它执行的一个过程
      [18468] 16 Oct 19:53:18.386 # +sdown master myredis 127.0.0.1 6379
      [18468] 16 Oct 19:56:16.306 # +odown master myredis 127.0.0.1 6379 #quorum 1/1
      [18468] 16 Oct 19:56:17.442 # +new-epoch 1
      [18468] 16 Oct 19:56:49.600 # +try-failover master myredis 127.0.0.1 6379		# 开始尝试故障转移
      [18468] 16 Oct 19:56:49.602 # +vote-for-leader 74535147bc67d3cec2bca767558b8e6602940710 1
      [18468] 16 Oct 19:56:49.602 # +elected-leader master myredis 127.0.0.1 6379
      [18468] 16 Oct 19:56:49.603 # +failover-state-select-slave master myredis 127.0.0.1 6379
      [18468] 16 Oct 19:56:49.686 # +tilt #tilt mode entered
      [18468] 16 Oct 19:57:19.707 # -tilt #tilt mode exited
      [18468] 16 Oct 19:57:19.708 # +selected-slave slave 127.0.0.1:6380 127.0.0.1 6380 @ myredis 127.0.0.1 6379	# 这里就开始选举出来了新的主机
      [18468] 16 Oct 19:57:19.709 * +failover-state-send-slaveof-noone slave 127.0.0.1:6380 127.0.0.1 6380 @ myredis 127.0.0.1 6379
      [18468] 16 Oct 19:57:19.770 * +failover-state-wait-promotion slave 127.0.0.1:6380 127.0.0.1 6380 @ myredis 127.0.0.1 6379
      [18468] 16 Oct 19:57:20.327 # +promoted-slave slave 127.0.0.1:6380 127.0.0.1 6380 @ myredis 127.0.0.1 6379
      [18468] 16 Oct 19:57:20.328 # +failover-state-reconf-slaves master myredis 127.0.0.1 6379
      [18468] 16 Oct 19:57:20.390 # +failover-end master myredis 127.0.0.1 6379
      [18468] 16 Oct 19:57:20.391 # +switch-master myredis 127.0.0.1 6379 127.0.0.1 6380
      [18468] 16 Oct 19:57:20.392 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ myredis 127.0.0.1 6380	# 这里就开始通知其他的从节点由之前的主机转为现在的主机
      [18468] 16 Oct 19:57:20.393 * +slave slave 127.0.0.1:6379 127.0.0.1 6379 @ myredis 127.0.0.1 6380
      [18468] 16 Oct 19:57:50.454 # +sdown slave 127.0.0.1:6379 127.0.0.1 6379 @ myredis 127.0.0.1 6380
      [18468] 16 Oct 19:57:50.455 # +sdown slave 127.0.0.1:6381 127.0.0.1 6381 @ myredis 127.0.0.1 6380
      ```

   2. 此时我们得知选举出主机的是6380,那么此刻我们去6380看看是否正确的切换为主机了!

      ```bash
      127.0.0.1:6380> info replication
      # Replication
      role:master
      connected_slaves:1
      slave0:ip=127.0.0.1,port=6381,state=online,offset=2910,lag=0			# 可以看到它管理的从机
      master_repl_offset:2910
      repl_backlog_active:1
      repl_backlog_size:1048576
      repl_backlog_first_byte_offset:1276
      repl_backlog_histlen:1635
      ```

   > 此时,我们在测试一下,我们上线原来的主机,看看会有什么效果:

   上线后,我们看哨兵日志:

   ```bash
   [18468] 16 Oct 20:12:58.125 # -sdown slave 127.0.0.1:6379 127.0.0.1 6379 @ myredis 127.0.0.1 6380		# 可以看出哨兵自动检测并转换为6380主机的从机
   [18468] 16 Oct 20:13:08.070 * +convert-to-slave slave 127.0.0.1:6379 127.0.0.1 6379 @ myredis 127.0.0.1 6380
   ```

   然后我们在6379这个旧的主机上看看信息:

   ```bash
   127.0.0.1:6379> info replication
   # Replication
   role:slave							# 发现已经更改为从机了!
   master_host:127.0.0.1
   master_port:6380					# 是6380的主机!
   master_link_status:up
   master_last_io_seconds_ago:1
   master_sync_in_progress:0
   slave_repl_offset:14641
   slave_priority:100
   slave_read_only:1
   connected_slaves:0
   master_repl_offset:0
   repl_backlog_active:0
   repl_backlog_size:1048576
   repl_backlog_first_byte_offset:0
   repl_backlog_histlen:0
   ```

   我们再去6380的主机:

   ```bash
   127.0.0.1:6380> info replication
   # Replication
   role:master
   connected_slaves:2
   slave0:ip=127.0.0.1,port=6381,state=online,offset=19227,lag=0			# 可以看出已经加入到这个新主机下的从节点了!
   slave1:ip=127.0.0.1,port=6379,state=online,offset=19227,lag=1
   master_repl_offset:19359
   repl_backlog_active:1
   repl_backlog_size:1048576
   repl_backlog_first_byte_offset:1276
   repl_backlog_histlen:18084
   ```

   

**结论**:如果主机此时回来了,也只能归并到新的主机下,当做从机,这就是哨兵模式的规则!

### 10.3 优缺点

优点:

1. 哨兵集群,基于主从复制模式,所有的主从配置的优点它都有!
2. 主从可以切换,故障可以转移,系统的可用性就会更好
3. 哨兵模式就是主从模式的升级,手动到自动,更加健壮!

缺点:

1. Redis不好在线扩容的,集群的容量一旦到达上限,在线扩容就变得十分麻烦!
2. 实现哨兵模式的配置其实很麻烦的,里面有很多选择!

> 哨兵模式的全部配置!

```bash
# Example sentinel.conf

#哨兵 sentinel实例运行的端口默认26379,如果有哨兵集群,我们还需要配置每个哨兵的的端口
port 26379

#哨兵 sentinel的工作目录
dir/tmp

# 哨兵 dentinel监控的 redis主节点的 ip port
# master-name 可以自己命名的主节点名字  只能由字母A-z、数字0-9、这三个字符".-_"组成
# quorum配置多少个sentinel哨兵统一认为 master主节点失联,那么这时客观上认为节点失联了
# sentinel monitor <master-name> <ip> <redis-port> <quorum>
sentinel monitor mymaster 127.0.0.1 6379 2

# 当在 Redis实例中开启了 requirepass foobared授权密码 这样所有连接 Redis实例的客户端都要提供密码
# 设置哨兵 sentinel连接主从的密码 注意必须为主从设置一样的验证密码
# sentinel auth-pass <master-name> <password>
sentinel auth-pass mymaster MysUPER--secret-0123password

# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时哨兵主观上认为主节点下线 默认30秒
# sentinel down-after-milliseconds <master-name> <milliseconds>
sentinel down-after-milliseconds mymaster 30000

# 这个配置项指定了在发生fai1over主备切换时最多可以有多少个s1ave同时对新的 master进行同步，这个数字越小，完成fai1over所需的时间就越长，但是如果这个数字越大，就意味着越多的 slave因为 replication而不可用。可以通过将这个值设为1来保证每次只有一个s1ave处于不能处理命令请求的状态。
# sentinel parallel-syncs <master-name> <nums laves>
sentinel parallel-syncs mymaster 1

# 故障转移的超时时间 failover-tImeout可以用在以下这些方面：
# 1，同一个 dentinel对同一个 master两次fai1over之间的间隔时间。
# 2，当一个slave从一个错误的 master那里同步数据开始计算时间。直到slave被纠正为向正确的 master那里同步数据时。
# 3.当想要取消一个正在进行的failover所需要的时间
# 4.当进行 failover时，配置所有slaves指向新的 master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按 parallel-syncs所配置的规则来了
# 默认三分钟的故障转移
# sentinel failover-timeout <master-name> <milliseconds>
sentinel failover-timeout mymaster 180000

# SCRIPTS EXECUTION
# 配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。
# 井对于脚本的运行结果有以下规则：
# 若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10
# 若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。
# 如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。

# 通知型脚本：当 dentinel有任何警告级别的事件发生时（比如说 redis实例的主观失效和客观失效等等），将会去调用这个脚本，这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数，一个是事件的类型，个是事件的描述。如果 sentinel.conf配置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。
# 通知脚本
# sentinel notification-script <master-name> <script-path>
sentinel notification-script mymaster /var/redis/notify.sh

# 客户端重新配置主节点参数脚本
# 当一个 master由于fai1over而发生改变时，这个脚本将会被调用，通知相关的客户端关于 master地址已经发生改变的信息。
# 以下参数将会在调用脚本时传给脚本
# <master-name> <role> <state> <from-ip> <from-port> <to-ip><to-port>
# 目前< state>总是“fai1over井<ro1e>是“leader"或者“observer"中的一个。
# 参数from-ip，from-port，to-ip，to-port是用来和旧的 master和新的 master（即旧的s1ave）通信的这个脚本应该是通用的，能被多次调用，不是针对性的。
#sentinel client-reconfig-script <master-name> <script-path>
sentinel client-reconfig-script mymaster /var/redis/reconfig.sh
```



## 11.Redis缓存穿透和雪崩（面试）

Redis缓存的使用,极大的提升了应用程序的性能和效率特别是数据查询方面,但同时,它也带来了一些问题。其中，最要害的问题，就是数据一致性问题，从严格意义上讲，这个问题无解。如果要对数据的一致性要求很高，那么就不能使用缓存！

另外的一些典型问题就是：缓存穿透、缓存雪崩和缓存击穿，目前，业界也都有比较流行的解决方案。

### 11.1 缓存穿透（大面积查询不到）

> 概念

缓存穿透的概念很简单,用户想要查询一个数据,发现redis内存数据库没有,也就是缓存没有命中,于是向持久层数据库查询,发现也没有,于是本次查询失败!当用户很多的时候,缓存都没有命中(比如秒杀),于是都去请求了持久层数据库,这会给持久层数据库造成很大的压力,这时候就相当于出现了缓存穿透!

画图理解：

![image-20211016205959215](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016205959215.png)

意思就是说当用户以读的请求先去缓存中查询user1，此时Redis缓存中有那么直接返回,而如果说查询的是User2,Redis缓存中没有,那么就会直接去数据库中查询数据,而数据库中也没有,那么此时2号用户就会不停的往MySQL中查询,因为Redis缓存,中查不到;如果一些恶意攻击者疯狂地向你服务器发送攻击,那么此时MySQL就会收到大量的洪水攻击,这个时候MySQL就会变得非常的容易崩溃!那么这个就是标准的缓存穿透!

其实就是避免大量的请求同时写入MySQL,瞬间这个MySQL就被冲垮!直接服务器宕机!MySQL崩了,导致整个系统雪崩!整个一些并发几十万几百万的那种!就是Redis缓存没找到就直接跑去找MySQL!

而所谓的缓存穿透,可理解为原本缓存就是MySQL上面的保护膜,然后直接被穿透直接去找MySQL了!

> 解决方案

**布隆过滤器**

布隆过滤器是一种数据结构,对所有可能查询的参数以hash形式存储,在控制层先进行校验,不符合则丢弃,从而避免了对底层存储系统的查询压力;

![image-20211016205554797](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016205554797.png)

​	**缓存空对象**

​	当存储层不命中后,即使返回的空对象也将其缓存起来,同时会设置一个过期时间,之后再访问这个数据库将会从缓存中获取,保护了后端数据源;

![image-20211016211435241](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016211435241.png)

但是这种方法会存在两个问题:

1. 如果空值能够被缓存起来,这就意味着缓存需要更多的空间存储更多的键,因为当中可能会有很多的空值的键,没有什么意义;
2. 即使对空值设置了过期时间,还是会存在缓存层和存储层的数据会有一段时间窗口的不一致,这对于需要保持一致性的业务会有影响!就是缓存层没有,但是存储层有!



### 11.2 缓存击穿（查询量，缓存过期！）

> 概述:

这里需要注意和缓存击穿的区别:缓存击穿,是指一个key非常热点,在不停的扛着大并发,大并发集中对这一个点进行访问,当这个key在失效的瞬间,持续的大并发就穿破缓存,直接请求数据库,就像在一个屏障上凿开了一个洞。

当某个key在过期的瞬间，有大量的请求并发访问，这类数据一般是热点数据，由于缓存过期，会同时访问数据库来查询最新数据，并且回写缓存，会导致数据库瞬间压力过大！

比如：微博服务器宕机（出现热点词，如xxx明星出轨），就是所有人都在请求一个位置，所有的流量都砸到这个点上，比如这个key设置60s过期，而我们设置60.1s恢复这个过期时间，而这0.1秒的过程由于巨大的冲击量瞬间击穿了缓存，就好比是在屏障上凿开了洞，直接砸向数据库，如果没有抗住，那么就会出现宕机这个情况！

> 解决方案

1. 设置热点数据永不过期
   - 从缓存层面来看，没有设置过期时间，就不会出现热点key过期后产生的问题，不过如果不设置过期时间就会一直缓存，造成缓存上限清理策略，所以这种方案不是很好！
2. 加互斥锁
   - 分布式锁：使用分布式锁，保证对于`每个key同一时间只有一个线程去查询后端服务`，其他线程没有获得分布式锁的权限，因此只需要等待即可，这种方式高并发的压力转移到了分布式锁，因为对分布式锁的考验很大！使用我们之前用过的命令。`setnx`,不存在即创建!

![image-20211016222004265](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016222004265.png)

### 11.3 缓存雪崩(集体失效)

> 概念

缓存雪崩:指在某一个时间段,缓存集中过期失效。或者说Redis集群宕机,也就是停电了!

产生雪崩的原因之一：

​	比如在写本文的时候，马上就到双十一零点的时候，很快就很迎来一波抢购，这波商品时间比较集中的放入了缓存，假设缓存一个小时，那么到了凌晨一点钟的时候，这批商品的缓存就都过期了。而对这批商品的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰，于是所有的请求都会到达存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况！

![image-20211016222745301](https://gitee.com/miawei/pic-go-img/raw/master/imgs/image-20211016222745301.png)

其实集中过期,倒不是非常致命,比较致命的缓存雪崩,是缓存服务器某个节点宕机或断网。因为自然形成的缓存雪崩，一定是在某个时间段集中创建缓存，这个时候，数据库也是可以顶住压力的。无非就是对数据库产生周期性的压力而已，而缓存服务节点的宕机，对数据库服务器造成的压力是不可预知的，很有可能瞬间就把数据库压垮！

例子：

双十一：停掉一些服务（保证主要的服务可用），比如说退款，你当天去退款很有可能就提示当天业务繁忙不能退款，请明天再试；

> 解决方案

1. redis高可用
   - 这个思想的含义是:既然redis有可能挂掉,那我多增几台redis,这样一台挂掉之后其他的还可以继续工作,其实就是搭建的集群(异地多活!)
2. 限流降级
   - 这个解决方案的思想是:在缓存失效后,通过加锁或者队列来`控制读数据库写缓存的线程数量`,比如对某个key只允许一个线程查询数据和写缓存,其他线程等待
3. 数据预热
   - 数据预热的含义就是在正式部署之前,我先把可能的数据先预先访问一遍,这样部分可能大量访问的数据就会加载到缓存中,在即将发生大并发访问前手动触发加载缓存不同的key,设置不同的过期时间,让缓存失效的时间点`尽量均匀`!

